
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>第11章：数据集（Datasets） &#8212; LLM-101创造营</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=40d2fe7a"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/11_datasets/chapter11_datasets';</script>
    <link rel="icon" href="../../_static/panda.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="第12章：推理 I：KV缓存（KV-Cache）" href="../12_inference_kv_cache/chapter12_inference_kv_cache.html" />
    <link rel="prev" title="第10章：速度提升III：分布式(Distributed)" href="../10_need_for_speed_iii_distributed/chapter10_need_for_speed_iii_distributed.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/llm101.png" class="logo__image only-light" alt="LLM-101创造营 - Home"/>
    <script>document.write(`<img src="../../_static/llm101.png" class="logo__image only-dark" alt="LLM-101创造营 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">目录</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01_bigram/chapter01_bigram_language_model.html">第01章：Bigram语言模型（语言建模）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_micrograd/chapter02_micrograd.html">第02章：Micrograd（机器学习，反向传播）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_ngram_model/chapter03_ngram_model.html">第03章：N-gram模型（多层感知器，矩阵乘法，GELU激活函数）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_attention/chapter04_attention_model.html">第04章：注意力机制（Attention，Softmax，位置编码器）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_transformer/chapter05_transformer.html">第05章：Transformer（transformer架构，残差连接，层归一化，GPT-2）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_tokenization/chaptet06_tokenization.html">第6章：分词技术(Tokenization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_optimization/chapter07_optimization.html">第7章：优化技术(Optimization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_need_for_speed_i_device/chapter08_need_for_speed_i_device.html">第8章：速度提升I：设备(Device)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09_need_for_speed_ii_precision/chapter09_need_for_speed_ii_precision.html">第9章：速度提升II：精度(Precision)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_need_for_speed_iii_distributed/chapter10_need_for_speed_iii_distributed.html">第10章：速度提升III：分布式(Distributed)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">第11章：数据集（Datasets）</a></li>

<li class="toctree-l1"><a class="reference internal" href="../12_inference_kv_cache/chapter12_inference_kv_cache.html">第12章：推理 I：KV缓存（KV-Cache）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_inference_quantization/chapter13_inference_quantization.html">第13章：推理 II：量化 (Quantization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_1_supervised_finetuning_basics.html">第14章：监督式微调 I-SFT-14.1 监督式微调基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_2_parmeter_efficient_finetuning.html">第14章：监督式微调 I: SFT-14.1 监督式微调基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_3_lora_technique.html">第14章：监督式微调 I: SFT-14.3 LoRA技术详解</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_4_chat_model_finetuning.html">第14章：监督式微调 I: SFT-14.4 聊天模型的监督式微调</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_5_practical_case_study.html">第14章：监督式微调 I: SFT-实践案例：故事讲述模型的SFT实现</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_Finetuning_ii_rl/chapter15_1_reinforcement_learning_basic.html">第15章：强化学习微调 II: RL-15.1 强化学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_Finetuning_ii_rl/chapter15_2_rlhf.html">第15章：强化学习微调 II: RL-15.2 人类反馈的强化学习(RLHF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_Finetuning_ii_rl/chapter15_3_ppo_algorithm.html">第15章：强化学习微调 II: RL-15.3 近端策略优化(PPO)算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_Finetuning_ii_rl/chapter15_4_dpo_algorithm.html">第15章：强化学习微调 II: RL-## 15.4 直接偏好优化(DPO)算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16_deployment/chapter16_1_api_development.html">第16章：部署-16.1 API开发基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16_deployment/chapter16_2_web_application.html">第16章：部署-16.2 Web应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_1_multimodal_basics.html">第17章：多模态-17.1 多模态基础理论</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_2_vqvae_technique.html">第17章：多模态-17.2 VQVAE技术详解</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_3_diffusion_transformer.html">第17章：多模态-17.3 扩散变换器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_4_lora_multimodal_training.html">第17章：多模态-基于LoRA的多模态模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_5_multimodal_model_integration.html">第17章：多模态-17.5 多模态模型整合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/00_appendix_intro.html">附录</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/A1_programming_languages.html">附录A：编程语言基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/B1_data_types.html">附录B：数据类型基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/C1_tensor_operations.html">附录C：张量操作基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/D1_deep_learning_frameworks.html">附录D：深度学习框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/E1_neural_network_architectures.html">附录E：神经网络架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/F1_multimodal.html">附录F：多模态基础</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/isLinXu/LLM-101-Bootcamp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/LLM-101-Bootcamp/edit/main/chapters/11_datasets/chapter11_datasets.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/LLM-101-Bootcamp/issues/new?title=Issue%20on%20page%20%2Fchapters/11_datasets/chapter11_datasets.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chapters/11_datasets/chapter11_datasets.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>第11章：数据集（Datasets）</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">第11章：数据集（Datasets）</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">11.1 数据集概述</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">11.2 数据收集</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">11.2.1 公开数据集</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">11.2.2 自定义数据收集</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">11.3 数据清洗与预处理</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">11.3.1 基础文本清洗</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">11.3.2 语言学预处理</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">11.3.3 故事特定的预处理</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">11.3.4 数据质量控制</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">11.4 数据加载与处理</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">11.4.1 数据格式化</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">11.4.2 数据分割</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">11.4.3 数据加载库</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">11.4.4 数据批处理与动态加载</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">11.5 合成数据生成</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">11.5.1 基于规则的数据生成</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">11.5.2 基于模型的数据生成</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">11.5.3 数据增强技术</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">11.5.4 合成数据的质量控制</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">11.6 数据集管理与版本控制</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">11.6.1 数据集元数据</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">11.6.2 数据集版本控制</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">11.6.3 数据集共享与发布</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">引用</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#readme-md">上传README.md</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">11.7 数据集评估与分析</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="datasets">
<h1>第11章：数据集（Datasets）<a class="headerlink" href="#datasets" title="Link to this heading">#</a></h1>
<section id="id1">
<h2>11.1 数据集概述<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>在构建故事讲述AI大语言模型的过程中，数据集的质量和规模直接决定了模型的性能和能力。大语言模型（LLM）需要海量的文本数据来学习语言的结构、语法、语义以及各种知识。对于专门用于讲故事的AI模型，我们需要特别关注那些包含丰富叙事结构、情节发展和人物塑造的文本数据。</p>
<p>语言模型的训练数据通常来源广泛，包括但不限于书籍、文章、网页内容、对话记录等。这些数据经过精心筛选和处理后，才能用于模型的训练。在本章中，我们将深入探讨数据集的收集、处理、加载以及合成数据生成的方法，为构建一个高质量的故事讲述AI模型奠定基础。</p>
<p>数据集的构建过程可以分为几个关键步骤：数据收集、数据清洗、数据标注、数据分割以及数据加载。每一步都至关重要，任何一个环节出现问题都可能导致最终模型性能的下降。因此，我们需要投入足够的时间和精力来确保数据集的质量。</p>
</section>
<section id="id2">
<h2>11.2 数据收集<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<section id="id3">
<h3>11.2.1 公开数据集<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>在开始构建自己的数据集之前，了解一些已有的公开数据集是非常有价值的。这些数据集不仅可以直接用于模型训练，还可以作为我们构建自己数据集的参考。以下是一些适用于故事讲述AI模型的公开数据集：</p>
<ol class="arabic simple">
<li><p><strong>BookCorpus</strong>：包含超过11,000本未出版的书籍，涵盖了各种类型的小说，是训练语言模型的优质资源。</p></li>
<li><p><strong>Project Gutenberg</strong>：提供了超过60,000本公版书籍的电子版本，包括大量经典文学作品。</p></li>
<li><p><strong>Wikitext</strong>：从维基百科文章中提取的大规模语言建模数据集，包含了丰富的知识和叙事内容。</p></li>
<li><p><strong>Children’s Book Test (CBT)</strong>：专门用于评估模型理解儿童故事能力的数据集，对于构建故事讲述AI特别有价值。</p></li>
<li><p><strong>ROCStories</strong>：包含50,000个日常生活的短故事，每个故事由五个句子组成，具有连贯的情节发展。</p></li>
<li><p><strong>WritingPrompts</strong>：来自Reddit的写作提示和相应的故事，包含了各种创意写作内容。</p></li>
<li><p><strong>LAMBADA</strong>：专门设计用于测试模型长距离依赖理解能力的数据集，对于故事生成尤为重要。</p></li>
</ol>
<p>这些公开数据集可以通过各种渠道获取，如Hugging Face的Datasets库、TensorFlow Datasets、Kaggle等平台。在实际应用中，我们通常会结合多个数据集，以确保模型能够学习到多样化的语言表达和叙事结构。</p>
</section>
<section id="id4">
<h3>11.2.2 自定义数据收集<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>除了使用公开数据集外，针对特定的故事类型或风格，我们可能需要收集自定义数据。以下是一些自定义数据收集的方法：</p>
<ol class="arabic simple">
<li><p><strong>网络爬虫</strong>：通过编写爬虫程序，从特定网站（如故事分享平台、文学网站等）收集故事内容。这种方法可以获取大量的数据，但需要注意版权问题和数据质量控制。</p></li>
<li><p><strong>API访问</strong>：许多平台提供API接口，允许开发者以结构化的方式获取内容。例如，使用Reddit API获取WritingPrompts子版块的内容，或使用News API获取新闻故事。</p></li>
<li><p><strong>合作收集</strong>：与作家、出版社或教育机构合作，获取专业创作的故事内容。这种方法可以获取高质量的数据，但可能需要支付费用或签订协议。</p></li>
<li><p><strong>众包平台</strong>：通过众包平台（如Amazon Mechanical Turk、Prolific等）招募人员创作或收集故事。这种方法可以快速获取大量数据，但需要设计良好的任务指南和质量控制机制。</p></li>
</ol>
<p>无论采用哪种方法，都需要确保收集的数据符合法律和伦理要求，并尊重原创作者的权益。同时，建立一个明确的数据收集计划，包括目标数据量、数据类型、质量标准等，可以帮助我们更有效地进行数据收集工作。</p>
</section>
</section>
<section id="id5">
<h2>11.3 数据清洗与预处理<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>收集到的原始数据通常包含各种噪声和不规则内容，需要进行清洗和预处理才能用于模型训练。数据清洗是提高模型训练效果的关键步骤，包括以下几个方面：</p>
<section id="id6">
<h3>11.3.1 基础文本清洗<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>基础文本清洗主要处理文本中的格式问题和明显的噪声：</p>
<ol class="arabic simple">
<li><p><strong>去除HTML标签</strong>：如果数据来源于网页，可能包含HTML标签，需要使用正则表达式或专门的库（如BeautifulSoup）去除。</p></li>
<li><p><strong>统一编码</strong>：确保所有文本使用相同的编码（通常是UTF-8），避免因编码不一致导致的乱码问题。</p></li>
<li><p><strong>处理特殊字符</strong>：根据需要保留或去除特殊字符、表情符号等。</p></li>
<li><p><strong>规范化空白字符</strong>：处理多余的空格、制表符、换行符等，使文本格式一致。</p></li>
<li><p><strong>大小写处理</strong>：根据需要统一文本的大小写，或保持原有格式。</p></li>
</ol>
<p>以下是一个简单的Python代码示例，展示了基础文本清洗的过程：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>

<span class="k">def</span> <span class="nf">basic_clean</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># 去除HTML标签</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="s2">&quot;html.parser&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get_text</span><span class="p">()</span>
    
    <span class="c1"># 处理特殊字符和多余空白</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\s+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>  <span class="c1"># 将多个空白字符替换为单个空格</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>  <span class="c1"># 去除首尾空白</span>
    
    <span class="c1"># 其他清洗步骤...</span>
    
    <span class="k">return</span> <span class="n">text</span>

<span class="c1"># 应用到数据集</span>
<span class="n">cleaned_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">basic_clean</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">raw_texts</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="id7">
<h3>11.3.2 语言学预处理<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>语言学预处理涉及更深层次的文本分析和处理：</p>
<ol class="arabic simple">
<li><p><strong>分词</strong>：将文本分割成单词或子词单元，这是后续处理的基础。</p></li>
<li><p><strong>词干提取和词形还原</strong>：将单词转换为其基本形式，减少词汇的变化形式。</p></li>
<li><p><strong>去除停用词</strong>：根据需要去除常见但信息量较少的词（如”the”、”and”等）。</p></li>
<li><p><strong>句子分割</strong>：将文本分割成句子单元，便于后续的处理和分析。</p></li>
</ol>
<p>以下是使用NLTK库进行语言学预处理的示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span><span class="p">,</span> <span class="n">sent_tokenize</span>
<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span><span class="p">,</span> <span class="n">WordNetLemmatizer</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>

<span class="c1"># 下载必要的资源</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">linguistic_preprocess</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># 句子分割</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    
    <span class="c1"># 分词、词形还原和去除停用词</span>
    <span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
    <span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
    
    <span class="n">processed_sentences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
        <span class="c1"># 分词</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
        
        <span class="c1"># 词形还原和去除停用词</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
        
        <span class="n">processed_sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">processed_sentences</span>

<span class="c1"># 应用到数据集</span>
<span class="n">processed_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">linguistic_preprocess</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">cleaned_texts</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>11.3.3 故事特定的预处理<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>对于故事文本，我们可能需要进行一些特定的预处理：</p>
<ol class="arabic simple">
<li><p><strong>章节和段落识别</strong>：识别故事的章节和段落结构，保留这些信息以便模型学习叙事结构。</p></li>
<li><p><strong>对话提取</strong>：识别和标记故事中的对话内容，这对于模型学习人物对话风格很重要。</p></li>
<li><p><strong>情节标记</strong>：识别故事中的关键情节点，如开端、发展、高潮、结局等。</p></li>
<li><p><strong>人物识别</strong>：识别故事中的人物及其关系，这有助于模型理解人物互动和发展。</p></li>
</ol>
<p>这些特定的预处理通常需要结合规则和机器学习方法来实现，可能需要一定的人工标注工作。</p>
</section>
<section id="id9">
<h3>11.3.4 数据质量控制<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>数据清洗的最后一步是进行质量控制，确保处理后的数据符合预期：</p>
<ol class="arabic simple">
<li><p><strong>数据完整性检查</strong>：确保没有缺失或损坏的数据。</p></li>
<li><p><strong>长度过滤</strong>：过滤掉过短或过长的文本，保持数据的一致性。</p></li>
<li><p><strong>重复检测</strong>：识别并处理重复的内容，避免模型过度学习某些模式。</p></li>
<li><p><strong>语言检测</strong>：确保数据是目标语言（如中文），过滤掉其他语言的内容。</p></li>
<li><p><strong>内容审核</strong>：过滤不适当的内容，确保数据符合伦理和法律要求。</p></li>
</ol>
<p>以下是一个简单的数据质量控制示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langdetect</span> <span class="kn">import</span> <span class="n">detect</span>

<span class="k">def</span> <span class="nf">quality_control</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">min_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">filtered_texts</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
        <span class="c1"># 长度过滤</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">min_length</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="k">continue</span>
        
        <span class="c1"># 语言检测（确保是英文）</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">detect</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">!=</span> <span class="s1">&#39;en&#39;</span><span class="p">:</span>
                <span class="k">continue</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">continue</span>
        
        <span class="c1"># 其他质量控制步骤...</span>
        
        <span class="n">filtered_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">filtered_texts</span>

<span class="c1"># 应用到数据集</span>
<span class="n">quality_texts</span> <span class="o">=</span> <span class="n">quality_control</span><span class="p">(</span><span class="n">processed_texts</span><span class="p">)</span>
</pre></div>
</div>
<p>通过这些数据清洗和预处理步骤，我们可以将原始的、杂乱的文本数据转换为结构化、高质量的训练数据，为后续的模型训练奠定基础。</p>
</section>
</section>
<section id="id10">
<h2>11.4 数据加载与处理<a class="headerlink" href="#id10" title="Link to this heading">#</a></h2>
<p>在准备好高质量的数据集后，下一步是设计高效的数据加载和处理流程，以便将数据输入到模型中进行训练。现代深度学习框架提供了多种工具和库来简化这一过程。</p>
<section id="id11">
<h3>11.4.1 数据格式化<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p>首先，我们需要将清洗后的数据转换为适合模型训练的格式。对于语言模型，常见的格式包括：</p>
<ol class="arabic simple">
<li><p><strong>文本文件</strong>：每行一个样本或文档，简单直观但处理大规模数据时效率较低。</p></li>
<li><p><strong>JSON/JSONL</strong>：每个样本包含多个字段，如文本内容、元数据等，灵活性高。</p></li>
<li><p><strong>TFRecord/WebDataset</strong>：二进制格式，专为高效的数据加载和处理设计。</p></li>
<li><p><strong>HDF5</strong>：适合存储大规模、层次化的数据，支持随机访问。</p></li>
<li><p><strong>Parquet</strong>：列式存储格式，适合处理结构化数据。</p></li>
</ol>
<p>对于故事文本，我们可能会选择JSONL格式，每个样本包含故事文本、标题、作者、类型等信息。以下是一个示例：</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;title&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;小红帽&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;author&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;格林兄弟&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;童话&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;text&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;从前有一个可爱的小女孩...&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;title&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;灰姑娘&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;author&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;格林兄弟&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;童话&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;text&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;从前，有一个善良的女孩...&quot;</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id12">
<h3>11.4.2 数据分割<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<p>在模型训练前，我们通常需要将数据集分割为训练集、验证集和测试集：</p>
<ol class="arabic simple">
<li><p><strong>训练集</strong>：用于模型的参数学习，通常占总数据的70-80%。</p></li>
<li><p><strong>验证集</strong>：用于调整超参数和早停，通常占10-15%。</p></li>
<li><p><strong>测试集</strong>：用于评估最终模型性能，通常占10-15%。</p></li>
</ol>
<p>分割数据时，需要确保各个子集的分布相似，避免引入偏差。以下是使用scikit-learn进行数据分割的示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># 假设stories是我们的故事数据列表</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">temp_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">stories</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">val_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">temp_data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;训练集大小: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;验证集大小: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">val_data</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;测试集大小: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id13">
<h3>11.4.3 数据加载库<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>现代深度学习框架提供了专门的数据加载库，简化了数据处理流程：</p>
<ol class="arabic simple">
<li><p><strong>PyTorch DataLoader</strong>：PyTorch的数据加载工具，支持批处理、多进程加载和自定义数据转换。</p></li>
<li><p><strong>TensorFlow tf.data</strong>：TensorFlow的数据加载API，提供高效的数据处理和转换功能。</p></li>
<li><p><strong>Hugging Face Datasets</strong>：专为NLP任务设计的库，支持多种数据格式和处理操作。</p></li>
<li><p><strong>WebDataset</strong>：专为大规模数据设计的库，基于tar文件格式，支持流式处理。</p></li>
</ol>
<p>以下是使用Hugging Face Datasets加载和处理故事数据集的示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DatasetDict</span>

<span class="c1"># 创建数据集字典</span>
<span class="n">dataset_dict</span> <span class="o">=</span> <span class="n">DatasetDict</span><span class="p">({</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span>
    <span class="s1">&#39;validation&#39;</span><span class="p">:</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">val_data</span><span class="p">),</span>
    <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
<span class="p">})</span>

<span class="c1"># 查看数据集信息</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset_dict</span><span class="p">)</span>

<span class="c1"># 应用数据转换</span>
<span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">dataset_dict</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id14">
<h3>11.4.4 数据批处理与动态加载<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<p>对于大规模数据集，我们通常采用批处理和动态加载的方式，避免将整个数据集加载到内存中：</p>
<ol class="arabic simple">
<li><p><strong>批处理</strong>：将数据分成小批次进行处理，每次只加载一个批次的数据。</p></li>
<li><p><strong>动态加载</strong>：在训练过程中动态加载数据，而不是预先加载所有数据。</p></li>
<li><p><strong>预取</strong>：在处理当前批次的同时，预先加载下一批次的数据，减少等待时间。</p></li>
<li><p><strong>缓存</strong>：缓存频繁使用的数据，减少重复加载的开销。</p></li>
</ol>
<p>以下是使用PyTorch DataLoader进行批处理和动态加载的示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="k">class</span> <span class="nc">StoryDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stories</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stories</span> <span class="o">=</span> <span class="n">stories</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
    
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stories</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">story</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stories</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span>
            <span class="n">story</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span>
            <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
        <span class="p">)</span>
        
        <span class="c1"># 将字典中的所有张量转换为一维</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">encoding</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="c1"># 创建数据集和数据加载器</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">StoryDataset</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># 使用数据加载器进行训练</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
    <span class="c1"># 将数据移动到GPU</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    
    <span class="c1"># 模型前向传播、损失计算和反向传播</span>
    <span class="c1"># ...</span>
</pre></div>
</div>
<p>通过这种方式，我们可以高效地处理大规模数据集，同时保持内存使用在合理范围内。</p>
</section>
</section>
<section id="id15">
<h2>11.5 合成数据生成<a class="headerlink" href="#id15" title="Link to this heading">#</a></h2>
<p>除了收集和处理现有的数据外，合成数据生成是另一种增加训练数据的重要方法，特别是在特定领域的数据稀缺时。对于故事讲述AI模型，合成数据可以帮助增强模型在特定类型故事或特定叙事风格上的能力。</p>
<section id="id16">
<h3>11.5.1 基于规则的数据生成<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<p>基于规则的方法使用预定义的模板和规则来生成新的故事样本：</p>
<ol class="arabic simple">
<li><p><strong>故事模板</strong>：创建基本的故事结构模板，如”主角遇到问题→尝试解决→遇到障碍→最终解决”。</p></li>
<li><p><strong>元素替换</strong>：在模板中替换不同的角色、场景、问题等元素，生成多样化的故事。</p></li>
<li><p><strong>语法规则</strong>：使用形式语法或其他语言规则来生成符合特定结构的文本。</p></li>
</ol>
<p>以下是一个简单的基于模板的故事生成示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># 故事元素</span>
<span class="n">characters</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;小明&quot;</span><span class="p">,</span> <span class="s2">&quot;小红&quot;</span><span class="p">,</span> <span class="s2">&quot;小华&quot;</span><span class="p">,</span> <span class="s2">&quot;小丽&quot;</span><span class="p">]</span>
<span class="n">settings</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;森林&quot;</span><span class="p">,</span> <span class="s2">&quot;学校&quot;</span><span class="p">,</span> <span class="s2">&quot;城堡&quot;</span><span class="p">,</span> <span class="s2">&quot;海边&quot;</span><span class="p">]</span>
<span class="n">problems</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;迷路了&quot;</span><span class="p">,</span> <span class="s2">&quot;遇到了一只神秘动物&quot;</span><span class="p">,</span> <span class="s2">&quot;发现了一个神秘洞穴&quot;</span><span class="p">,</span> <span class="s2">&quot;遭遇了暴风雨&quot;</span><span class="p">]</span>
<span class="n">solutions</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;得到了朋友的帮助&quot;</span><span class="p">,</span> <span class="s2">&quot;凭借智慧解决了问题&quot;</span><span class="p">,</span> <span class="s2">&quot;找到了回家的路&quot;</span><span class="p">,</span> <span class="s2">&quot;学会了一项新技能&quot;</span><span class="p">]</span>

<span class="c1"># 故事模板</span>
<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;从前，</span><span class="si">{character}</span><span class="s2">在</span><span class="si">{setting}</span><span class="s2">里</span><span class="si">{problem}</span><span class="s2">。经过一番努力，最终</span><span class="si">{solution}</span><span class="s2">。&quot;</span>

<span class="c1"># 生成故事</span>
<span class="k">def</span> <span class="nf">generate_story</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">character</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">characters</span><span class="p">),</span>
        <span class="n">setting</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">settings</span><span class="p">),</span>
        <span class="n">problem</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">problems</span><span class="p">),</span>
        <span class="n">solution</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">solutions</span><span class="p">)</span>
    <span class="p">)</span>

<span class="c1"># 生成100个故事样本</span>
<span class="n">synthetic_stories</span> <span class="o">=</span> <span class="p">[</span><span class="n">generate_story</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)]</span>
</pre></div>
</div>
</section>
<section id="id17">
<h3>11.5.2 基于模型的数据生成<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<p>使用现有的语言模型来生成新的故事样本，这种方法可以产生更自然、多样化的文本：</p>
<ol class="arabic simple">
<li><p><strong>预训练模型生成</strong>：使用GPT、BART等预训练模型生成新的故事文本。</p></li>
<li><p><strong>条件生成</strong>：基于特定的提示、风格或主题生成故事。</p></li>
<li><p><strong>数据增强</strong>：对现有故事进行改写、扩展或变换，生成新的变体。</p></li>
</ol>
<p>以下是使用Hugging Face Transformers库进行基于模型的故事生成示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># 初始化文本生成管道</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;text-generation&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="c1"># 故事提示</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;从前有一个小女孩，她住在森林边缘，&quot;</span><span class="p">,</span>
    <span class="s2">&quot;在一个遥远的王国，有一位年轻的王子，&quot;</span><span class="p">,</span>
    <span class="s2">&quot;太空站的警报突然响起，宇航员们发现，&quot;</span>
<span class="p">]</span>

<span class="c1"># 生成故事</span>
<span class="n">synthetic_stories</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
    <span class="c1"># 为每个提示生成10个不同的故事</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">]</span>
        <span class="n">synthetic_stories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id18">
<h3>11.5.3 数据增强技术<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<p>数据增强是一种特殊的合成数据生成方法，通过对现有数据进行变换来创建新的样本：</p>
<ol class="arabic simple">
<li><p><strong>同义词替换</strong>：用同义词替换文本中的某些词，保持语义不变但创造新的表达。</p></li>
<li><p><strong>回译</strong>：将文本翻译成另一种语言，然后再翻译回来，产生表达方式的变化。</p></li>
<li><p><strong>句子重排</strong>：改变句子的顺序，创造不同的叙事流程。</p></li>
<li><p><strong>插入和删除</strong>：随机插入或删除某些词或短语，增加文本的多样性。</p></li>
<li><p><strong>EDA (Easy Data Augmentation)</strong>：结合上述多种方法的简单数据增强技术。</p></li>
</ol>
<p>以下是使用nlpaug库进行文本数据增强的示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nlpaug.augmenter.word</span> <span class="k">as</span> <span class="nn">naw</span>
<span class="kn">import</span> <span class="nn">nlpaug.augmenter.sentence</span> <span class="k">as</span> <span class="nn">nas</span>

<span class="c1"># 同义词替换增强器</span>
<span class="n">synonym_aug</span> <span class="o">=</span> <span class="n">naw</span><span class="o">.</span><span class="n">SynonymAug</span><span class="p">()</span>

<span class="c1"># 回译增强器</span>
<span class="n">back_translation_aug</span> <span class="o">=</span> <span class="n">naw</span><span class="o">.</span><span class="n">BackTranslationAug</span><span class="p">(</span>
    <span class="n">from_model_name</span><span class="o">=</span><span class="s1">&#39;facebook/wmt19-en-de&#39;</span><span class="p">,</span>
    <span class="n">to_model_name</span><span class="o">=</span><span class="s1">&#39;facebook/wmt19-de-en&#39;</span>
<span class="p">)</span>

<span class="c1"># 句子重排增强器</span>
<span class="n">sentence_shuffle_aug</span> <span class="o">=</span> <span class="n">nas</span><span class="o">.</span><span class="n">RandomSentAug</span><span class="p">()</span>

<span class="c1"># 应用数据增强</span>
<span class="n">augmented_stories</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">story</span> <span class="ow">in</span> <span class="n">stories</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>  <span class="c1"># 只对前10个故事进行增强示例</span>
    <span class="c1"># 同义词替换</span>
    <span class="n">aug_syn</span> <span class="o">=</span> <span class="n">synonym_aug</span><span class="o">.</span><span class="n">augment</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
    <span class="n">augmented_stories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aug_syn</span><span class="p">)</span>
    
    <span class="c1"># 回译</span>
    <span class="n">aug_back</span> <span class="o">=</span> <span class="n">back_translation_aug</span><span class="o">.</span><span class="n">augment</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
    <span class="n">augmented_stories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aug_back</span><span class="p">)</span>
    
    <span class="c1"># 句子重排</span>
    <span class="n">aug_shuffle</span> <span class="o">=</span> <span class="n">sentence_shuffle_aug</span><span class="o">.</span><span class="n">augment</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
    <span class="n">augmented_stories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">aug_shuffle</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id19">
<h3>11.5.4 合成数据的质量控制<a class="headerlink" href="#id19" title="Link to this heading">#</a></h3>
<p>合成数据生成后，同样需要进行质量控制，确保数据的质量和多样性：</p>
<ol class="arabic simple">
<li><p><strong>人工审核</strong>：对生成的样本进行抽样审核，确保质量和适当性。</p></li>
<li><p><strong>自动评估</strong>：使用自动化指标（如困惑度、BLEU分数等）评估生成文本的质量。</p></li>
<li><p><strong>多样性检查</strong>：确保生成的样本具有足够的多样性，避免重复和单一模式。</p></li>
<li><p><strong>与原始数据比较</strong>：确保合成数据与原始数据在分布上相似，但不是简单的复制。</p></li>
</ol>
<p>以下是一个简单的合成数据质量评估示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.translate.bleu_score</span> <span class="kn">import</span> <span class="n">sentence_bleu</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">evaluate_synthetic_data</span><span class="p">(</span><span class="n">original_stories</span><span class="p">,</span> <span class="n">synthetic_stories</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># 抽样原始故事和合成故事</span>
    <span class="n">orig_sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">original_stories</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">sample_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">original_stories</span><span class="p">)))</span>
    <span class="n">synth_sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">synthetic_stories</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">sample_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">synthetic_stories</span><span class="p">)))</span>
    
    <span class="c1"># 计算平均长度</span>
    <span class="n">orig_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">story</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">story</span> <span class="ow">in</span> <span class="n">orig_sample</span><span class="p">]</span>
    <span class="n">synth_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">story</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">story</span> <span class="ow">in</span> <span class="n">synth_sample</span><span class="p">]</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;原始故事平均长度: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">orig_lengths</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">orig_lengths</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;合成故事平均长度: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">synth_lengths</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">synth_lengths</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># 计算词汇多样性</span>
    <span class="n">orig_vocab</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">word</span> <span class="k">for</span> <span class="n">story</span> <span class="ow">in</span> <span class="n">orig_sample</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">story</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
    <span class="n">synth_vocab</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">word</span> <span class="k">for</span> <span class="n">story</span> <span class="ow">in</span> <span class="n">synth_sample</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">story</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;原始故事词汇量: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">orig_vocab</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;合成故事词汇量: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">synth_vocab</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># 计算BLEU分数（评估合成故事与原始故事的相似度）</span>
    <span class="n">bleu_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">synth_story</span> <span class="ow">in</span> <span class="n">synth_sample</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>  <span class="c1"># 只对前10个样本计算BLEU分数</span>
        <span class="n">synth_tokens</span> <span class="o">=</span> <span class="n">synth_story</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence_bleu</span><span class="p">([</span><span class="n">orig_story</span><span class="o">.</span><span class="n">split</span><span class="p">()],</span> <span class="n">synth_tokens</span><span class="p">)</span> <span class="k">for</span> <span class="n">orig_story</span> <span class="ow">in</span> <span class="n">orig_sample</span><span class="p">[:</span><span class="mi">10</span><span class="p">]]</span>
        <span class="n">bleu_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>  <span class="c1"># 取最相似的原始故事的BLEU分数</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;平均最大BLEU分数: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bleu_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># 其他评估指标...</span>

<span class="c1"># 评估合成数据</span>
<span class="n">evaluate_synthetic_data</span><span class="p">(</span><span class="n">original_stories</span><span class="p">,</span> <span class="n">synthetic_stories</span><span class="p">)</span>
</pre></div>
</div>
<p>通过合成数据生成，我们可以显著扩充训练数据集，特别是在特定领域或风格的数据稀缺时。但需要注意的是，合成数据应该作为原始数据的补充，而不是替代，两者结合使用通常能获得最佳效果。</p>
</section>
</section>
<section id="id20">
<h2>11.6 数据集管理与版本控制<a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<p>随着项目的发展，数据集可能会不断更新和扩充，因此建立良好的数据集管理和版本控制机制非常重要。</p>
<section id="id21">
<h3>11.6.1 数据集元数据<a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
<p>为数据集创建详细的元数据，记录数据集的来源、处理方法、统计信息等：</p>
<ol class="arabic simple">
<li><p><strong>基本信息</strong>：数据集名称、版本、创建日期、作者等。</p></li>
<li><p><strong>来源信息</strong>：数据的来源、收集方法、原始格式等。</p></li>
<li><p><strong>处理信息</strong>：清洗和预处理的步骤、使用的工具和参数等。</p></li>
<li><p><strong>统计信息</strong>：样本数量、词汇量、平均长度、类别分布等。</p></li>
<li><p><strong>使用说明</strong>：数据集的适用场景、限制条件、许可证信息等。</p></li>
</ol>
<p>以下是一个数据集元数据的JSON示例：</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;StoryTeller Dataset&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1.0.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;created_at&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2023-05-15&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;authors&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;张三&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;李四&quot;</span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;description&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;用于训练故事讲述AI模型的中文故事数据集&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;sources&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Project Gutenberg&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;url&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;https://www.gutenberg.org/&quot;</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;中国民间故事集&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;book&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;publisher&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;人民文学出版社&quot;</span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;processing&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="nt">&quot;step&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HTML清洗&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;tool&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;BeautifulSoup&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;4.9.3&quot;</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="nt">&quot;step&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;分词&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;tool&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;jieba&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0.42.1&quot;</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="nt">&quot;step&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;质量过滤&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;min_length&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;max_length&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5000</span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;statistics&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;total_samples&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">10000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;train_samples&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">8000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;val_samples&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;test_samples&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;avg_length&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1250</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;vocabulary_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">35000</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;genres&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;童话&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3000</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;民间故事&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2500</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;寓言&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1500</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;科幻&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1500</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;其他&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1500</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;license&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;CC BY-NC-SA 4.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;usage_notes&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;本数据集仅用于研究和非商业用途。使用时请注明出处。&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="id22">
<h3>11.6.2 数据集版本控制<a class="headerlink" href="#id22" title="Link to this heading">#</a></h3>
<p>随着项目的发展，数据集可能会经历多次更新和迭代，建立版本控制机制可以帮助追踪这些变化：</p>
<ol class="arabic simple">
<li><p><strong>语义化版本号</strong>：使用类似软件的版本号系统（如1.0.0、1.1.0等）来标识不同版本的数据集。</p></li>
<li><p><strong>变更日志</strong>：记录每个版本的变更内容，包括新增、修改和删除的数据。</p></li>
<li><p><strong>数据集快照</strong>：为每个版本创建不可变的快照，确保实验的可重复性。</p></li>
<li><p><strong>Git LFS</strong>：使用Git Large File Storage等工具进行数据集的版本控制。</p></li>
<li><p><strong>数据集注册表</strong>：使用专门的数据集注册工具（如DVC、Weights &amp; Biases等）管理数据集版本。</p></li>
</ol>
<p>以下是使用DVC (Data Version Control) 进行数据集版本控制的示例：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 安装DVC</span>
pip<span class="w"> </span>install<span class="w"> </span>dvc

<span class="c1"># 初始化DVC仓库</span>
dvc<span class="w"> </span>init

<span class="c1"># 添加数据集到DVC跟踪</span>
dvc<span class="w"> </span>add<span class="w"> </span>data/storyteller_dataset_v1.0.0

<span class="c1"># 提交更改到Git</span>
git<span class="w"> </span>add<span class="w"> </span>data/storyteller_dataset_v1.0.0.dvc<span class="w"> </span>.gitignore
git<span class="w"> </span>commit<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;Add StoryTeller Dataset v1.0.0&quot;</span>

<span class="c1"># 修改数据集后，更新DVC跟踪</span>
dvc<span class="w"> </span>add<span class="w"> </span>data/storyteller_dataset_v1.1.0

<span class="c1"># 提交新版本到Git</span>
git<span class="w"> </span>add<span class="w"> </span>data/storyteller_dataset_v1.1.0.dvc
git<span class="w"> </span>commit<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;Update to StoryTeller Dataset v1.1.0&quot;</span>

<span class="c1"># 切换到特定版本的数据集</span>
git<span class="w"> </span>checkout<span class="w"> </span>&lt;commit_hash&gt;
dvc<span class="w"> </span>checkout
</pre></div>
</div>
</section>
<section id="id23">
<h3>11.6.3 数据集共享与发布<a class="headerlink" href="#id23" title="Link to this heading">#</a></h3>
<p>对于有价值的数据集，可以考虑共享和发布，使其能够被更广泛的研究社区使用：</p>
<ol class="arabic simple">
<li><p><strong>数据托管平台</strong>：使用Hugging Face Datasets、Kaggle、Zenodo等平台托管和分享数据集。</p></li>
<li><p><strong>文档和示例</strong>：提供详细的文档和使用示例，帮助其他研究者理解和使用数据集。</p></li>
<li><p><strong>引用信息</strong>：提供正确的引用格式，使其他研究者可以在论文中引用你的数据集。</p></li>
<li><p><strong>许可证</strong>：选择适当的许可证（如CC BY-NC-SA、MIT等），明确数据集的使用条件。</p></li>
</ol>
<p>以下是在Hugging Face Datasets上发布数据集的示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DatasetDict</span>
<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">HfApi</span>

<span class="c1"># 准备数据集</span>
<span class="n">dataset_dict</span> <span class="o">=</span> <span class="n">DatasetDict</span><span class="p">({</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span>
    <span class="s1">&#39;validation&#39;</span><span class="p">:</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">val_data</span><span class="p">),</span>
    <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
<span class="p">})</span>

<span class="c1"># 保存数据集到本地</span>
<span class="n">dataset_dict</span><span class="o">.</span><span class="n">save_to_disk</span><span class="p">(</span><span class="s2">&quot;storyteller_dataset&quot;</span><span class="p">)</span>

<span class="c1"># 登录Hugging Face</span>
<span class="n">api</span> <span class="o">=</span> <span class="n">HfApi</span><span class="p">()</span>
<span class="n">api</span><span class="o">.</span><span class="n">login</span><span class="p">()</span>

<span class="c1"># 上传数据集到Hugging Face Hub</span>
<span class="n">dataset_dict</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;username/storyteller_dataset&quot;</span><span class="p">,</span> <span class="n">private</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># 更新数据集卡片（README.md）</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;README.md&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2"># StoryTeller Dataset</span>

<span class="s2">用于训练故事讲述AI模型的中文故事数据集。</span>

<span class="s2">## 数据集描述</span>

<span class="s2">本数据集包含10,000个中文故事样本，涵盖童话、民间故事、寓言、科幻等多种类型。</span>

<span class="s2">## 使用方法</span>

<span class="s2">```python</span>
<span class="s2">from datasets import load_dataset</span>

<span class="s2">dataset = load_dataset(&quot;username/storyteller_dataset&quot;)</span>
</pre></div>
</div>
</section>
</section>
<section id="id24">
<h2>引用<a class="headerlink" href="#id24" title="Link to this heading">#</a></h2>
<p>如果您在研究中使用了本数据集，请引用：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataset</span><span class="p">{</span><span class="n">storyteller_dataset</span><span class="p">,</span>
  <span class="n">author</span> <span class="o">=</span> <span class="p">{</span><span class="n">张三</span><span class="p">,</span> <span class="n">李四</span><span class="p">},</span>
  <span class="n">title</span> <span class="o">=</span> <span class="p">{</span><span class="n">StoryTeller</span> <span class="n">Dataset</span><span class="p">},</span>
  <span class="n">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2023</span><span class="p">},</span>
  <span class="n">url</span> <span class="o">=</span> <span class="p">{</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="o">.</span><span class="n">co</span><span class="o">/</span><span class="n">datasets</span><span class="o">/</span><span class="n">username</span><span class="o">/</span><span class="n">storyteller_dataset</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="readme-md">
<h1>上传README.md<a class="headerlink" href="#readme-md" title="Link to this heading">#</a></h1>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">api</span><span class="o">.</span><span class="n">upload_file</span><span class="p">(</span>
    <span class="n">path_or_fileobj</span><span class="o">=</span><span class="s2">&quot;README.md&quot;</span><span class="p">,</span>
    <span class="n">path_in_repo</span><span class="o">=</span><span class="s2">&quot;README.md&quot;</span><span class="p">,</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;username/storyteller_dataset&quot;</span><span class="p">,</span>
    <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>通过良好的数据集管理和版本控制，我们可以确保数据集的质量、可追溯性和可重用性，为模型训练和研究提供坚实的基础。</p>
<section id="id25">
<h2>11.7 数据集评估与分析<a class="headerlink" href="#id25" title="Link to this heading">#</a></h2>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../10_need_for_speed_iii_distributed/chapter10_need_for_speed_iii_distributed.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">第10章：速度提升III：分布式(Distributed)</p>
      </div>
    </a>
    <a class="right-next"
       href="../12_inference_kv_cache/chapter12_inference_kv_cache.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">第12章：推理 I：KV缓存（KV-Cache）</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">第11章：数据集（Datasets）</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">11.1 数据集概述</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">11.2 数据收集</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">11.2.1 公开数据集</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">11.2.2 自定义数据收集</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">11.3 数据清洗与预处理</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">11.3.1 基础文本清洗</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">11.3.2 语言学预处理</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">11.3.3 故事特定的预处理</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">11.3.4 数据质量控制</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">11.4 数据加载与处理</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">11.4.1 数据格式化</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">11.4.2 数据分割</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">11.4.3 数据加载库</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">11.4.4 数据批处理与动态加载</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">11.5 合成数据生成</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">11.5.1 基于规则的数据生成</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">11.5.2 基于模型的数据生成</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">11.5.3 数据增强技术</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">11.5.4 合成数据的质量控制</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">11.6 数据集管理与版本控制</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">11.6.1 数据集元数据</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">11.6.2 数据集版本控制</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">11.6.3 数据集共享与发布</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">引用</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#readme-md">上传README.md</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">11.7 数据集评估与分析</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By isLinXu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, isLinXu.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>