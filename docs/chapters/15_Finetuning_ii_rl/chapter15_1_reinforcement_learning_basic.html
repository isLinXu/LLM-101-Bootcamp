
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>第15章：强化学习微调 II: RL-15.1 强化学习基础 &#8212; LLM-101创造营</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=40d2fe7a"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/mathjax_config.js?v=e9f8e615"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/15_Finetuning_ii_rl/chapter15_1_reinforcement_learning_basic';</script>
    <link rel="icon" href="../../_static/panda.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="第15章：强化学习微调 II: RL-15.2 人类反馈的强化学习(RLHF)" href="chapter15_2_rlhf.html" />
    <link rel="prev" title="第14章：监督式微调 I: SFT-实践案例：故事讲述模型的SFT实现" href="../14_finetuning_i_sft/chapter14_5_practical_case_study.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/llm101.png" class="logo__image only-light" alt="LLM-101创造营 - Home"/>
    <script>document.write(`<img src="../../_static/llm101.png" class="logo__image only-dark" alt="LLM-101创造营 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">目录</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01_bigram/chapter01_bigram_language_model.html">第01章：Bigram语言模型（语言建模）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_micrograd/chapter02_micrograd.html">第02章：Micrograd（机器学习，反向传播）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_ngram_model/chapter03_ngram_model.html">第03章：N-gram模型（多层感知器，矩阵乘法，GELU激活函数）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_attention/chapter04_attention_model.html">第04章：注意力机制（Attention，Softmax，位置编码器）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_transformer/chapter05_transformer.html">第05章：Transformer（transformer架构，残差连接，层归一化，GPT-2）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_tokenization/chaptet06_tokenization.html">第6章：分词技术(Tokenization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_optimization/chapter07_optimization.html">第7章：优化技术(Optimization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_need_for_speed_i_device/chapter08_need_for_speed_i_device.html">第8章：速度提升I：设备(Device)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09_need_for_speed_ii_precision/chapter09_need_for_speed_ii_precision.html">第9章：速度提升II：精度(Precision)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_need_for_speed_iii_distributed/chapter10_need_for_speed_iii_distributed.html">第10章：速度提升III：分布式(Distributed)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_datasets/chapter11_datasets.html">第11章：数据集（Datasets）</a></li>

<li class="toctree-l1"><a class="reference internal" href="../12_inference_kv_cache/chapter12_inference_kv_cache.html">第12章：推理 I：KV缓存（KV-Cache）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_inference_quantization/chapter13_inference_quantization.html">第13章：推理 II：量化 (Quantization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_1_supervised_finetuning_basics.html">第14章：监督式微调 I-SFT-14.1 监督式微调基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_2_parmeter_efficient_finetuning.html">第14章：监督式微调 I: SFT-14.1 监督式微调基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_3_lora_technique.html">第14章：监督式微调 I: SFT-14.3 LoRA技术详解</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_4_chat_model_finetuning.html">第14章：监督式微调 I: SFT-14.4 聊天模型的监督式微调</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_5_practical_case_study.html">第14章：监督式微调 I: SFT-实践案例：故事讲述模型的SFT实现</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">第15章：强化学习微调 II: RL-15.1 强化学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter15_2_rlhf.html">第15章：强化学习微调 II: RL-15.2 人类反馈的强化学习(RLHF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter15_3_ppo_algorithm.html">第15章：强化学习微调 II: RL-15.3 近端策略优化(PPO)算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter15_4_dpo_algorithm.html">第15章：强化学习微调 II: RL-## 15.4 直接偏好优化(DPO)算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16_deployment/chapter16_1_api_development.html">第16章：部署-16.1 API开发基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16_deployment/chapter16_2_web_application.html">第16章：部署-16.2 Web应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_1_multimodal_basics.html">第17章：多模态-17.1 多模态基础理论</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_2_vqvae_technique.html">第17章：多模态-17.2 VQVAE技术详解</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_3_diffusion_transformer.html">第17章：多模态-17.3 扩散变换器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_4_lora_multimodal_training.html">第17章：多模态-基于LoRA的多模态模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_5_multimodal_model_integration.html">第17章：多模态-17.5 多模态模型整合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/00_appendix_intro.html">附录</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/A1_programming_languages.html">附录A：编程语言基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/B1_data_types.html">附录B：数据类型基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/C1_tensor_operations.html">附录C：张量操作基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/D1_deep_learning_frameworks.html">附录D：深度学习框架</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/E1_neural_network_architectures.html">附录E：神经网络架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/F1_multimodal.html">附录F：多模态基础</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/isLinXu/LLM-101-Bootcamp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/LLM-101-Bootcamp/edit/main/chapters/15_Finetuning_ii_rl/chapter15_1_reinforcement_learning_basic.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/LLM-101-Bootcamp/issues/new?title=Issue%20on%20page%20%2Fchapters/15_Finetuning_ii_rl/chapter15_1_reinforcement_learning_basic.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chapters/15_Finetuning_ii_rl/chapter15_1_reinforcement_learning_basic.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>第15章：强化学习微调 II: RL-15.1 强化学习基础</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">15.1 强化学习基础</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">强化学习的核心概念</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp">强化学习在NLP中的应用挑战</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">奖励函数设计</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">奖励函数的类型</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">故事质量的评估维度</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">奖励函数实现示例</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">策略与价值函数</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#policy">策略（Policy）</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#value-function">价值函数（Value Function）</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">策略优化</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">探索与利用的平衡</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlnlp">RL在NLP中的应用挑战</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">实际应用示例</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">案例：优化童话故事的教育价值和吸引力</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="ii-rl-15-1">
<h1>第15章：强化学习微调 II: RL-15.1 强化学习基础<a class="headerlink" href="#ii-rl-15-1" title="Link to this heading">#</a></h1>
<section id="id1">
<h2>15.1 强化学习基础<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>在第14章中，我们探讨了如何通过监督式微调（SFT）使大型语言模型适应故事讲述任务。虽然SFT能够显著提升模型的能力，但它仍然存在一些固有的局限性。特别是，SFT仅能让模型学习模仿训练数据中的模式，而无法直接优化模型以满足人类的真实偏好或特定的质量标准。这就是强化学习（Reinforcement Learning，RL）微调发挥作用的地方。</p>
<p>强化学习微调允许我们根据特定的奖励信号来调整模型行为，使其生成的内容更符合我们的期望。在故事讲述AI的背景下，这意味着我们可以训练模型生成更有创意、更连贯、更符合特定风格或价值观的故事。本节将介绍强化学习的核心概念，为后续章节中的高级技术奠定基础。</p>
<section id="id2">
<h3>强化学习的核心概念<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>强化学习是机器学习的一个分支，它关注如何使智能体（agent）在与环境交互的过程中，通过试错学习来最大化累积奖励。与监督学习不同，强化学习不依赖于标记数据，而是通过奖励信号来指导学习过程。</p>
<p>在强化学习框架中，有几个关键概念：</p>
<ol class="arabic simple">
<li><p><strong>智能体（Agent）</strong>：做出决策的实体，在我们的情境中，这是故事讲述AI模型。</p></li>
<li><p><strong>环境（Environment）</strong>：智能体交互的外部系统，对于语言模型来说，这可以是模拟的对话环境或评估系统。</p></li>
<li><p><strong>状态（State）</strong>：环境的当前情况，对于故事生成任务，这可能是当前的故事上下文或提示。</p></li>
<li><p><strong>动作（Action）</strong>：智能体可以执行的操作，在语言生成中，这是模型生成的下一个词或句子。</p></li>
<li><p><strong>奖励（Reward）</strong>：环境对智能体动作的反馈，用于评估动作的好坏，例如故事质量的评分。</p></li>
<li><p><strong>策略（Policy）</strong>：智能体的决策规则，决定在给定状态下应该采取什么动作。对于语言模型，这是决定下一个词的概率分布。</p></li>
<li><p><strong>价值函数（Value Function）</strong>：估计在给定状态下，预期未来奖励的函数，帮助智能体评估不同状态的价值。</p></li>
<li><p><strong>轨迹（Trajectory）</strong>：一系列状态、动作和奖励的序列，代表智能体与环境交互的一次完整过程。</p></li>
</ol>
<p>强化学习的目标是找到一个最优策略，使智能体能够获得最大的累积奖励。在故事讲述AI的背景下，这意味着找到一种生成策略，使模型能够创作出获得最高人类评价的故事。</p>
</section>
<section id="nlp">
<h3>强化学习在NLP中的应用挑战<a class="headerlink" href="#nlp" title="Link to this heading">#</a></h3>
<p>虽然强化学习在游戏、机器人控制等领域取得了显著成功，但将其应用于自然语言处理（NLP）和大型语言模型面临一些独特的挑战：</p>
<ol class="arabic simple">
<li><p><strong>高维离散动作空间</strong>：</p>
<ul class="simple">
<li><p>语言模型的动作空间是词汇表的大小，通常包含数万到数十万个词元</p></li>
<li><p>这使得传统的强化学习算法难以有效探索所有可能的动作</p></li>
<li><p>需要特殊的技术来处理如此大的动作空间</p></li>
</ul>
</li>
<li><p><strong>稀疏奖励问题</strong>：</p>
<ul class="simple">
<li><p>在故事生成中，有意义的奖励通常只在完整故事生成后才能获得</p></li>
<li><p>中间步骤（单个词或句子）的贡献难以评估</p></li>
<li><p>这导致了信用分配问题：很难确定哪些特定决策导致了最终的好或坏结果</p></li>
</ul>
</li>
<li><p><strong>奖励设计的复杂性</strong>：</p>
<ul class="simple">
<li><p>故事质量是多维度的，包括创意性、连贯性、吸引力等</p></li>
<li><p>这些维度难以量化，且可能相互冲突</p></li>
<li><p>设计能够捕捉所有这些方面的奖励函数非常困难</p></li>
</ul>
</li>
<li><p><strong>样本效率低</strong>：</p>
<ul class="simple">
<li><p>传统强化学习算法需要大量样本才能收敛</p></li>
<li><p>在语言模型中，生成和评估每个样本的成本很高</p></li>
<li><p>这使得纯粹的试错学习在实践中不可行</p></li>
</ul>
</li>
<li><p><strong>不稳定性</strong>：</p>
<ul class="simple">
<li><p>语言生成的随机性使得训练过程更加不稳定</p></li>
<li><p>小的参数变化可能导致生成质量的显著波动</p></li>
<li><p>需要特殊的稳定化技术</p></li>
</ul>
</li>
<li><p><strong>人类偏好的主观性</strong>：</p>
<ul class="simple">
<li><p>不同人对故事质量的评价可能有很大差异</p></li>
<li><p>偏好可能随时间和上下文变化</p></li>
<li><p>这使得构建一致的奖励信号变得困难</p></li>
</ul>
</li>
</ol>
<p>为了应对这些挑战，研究人员开发了一系列专门针对语言模型的强化学习方法，其中最著名的是人类反馈的强化学习（RLHF），我们将在下一节详细讨论。</p>
</section>
<section id="id3">
<h3>奖励函数设计<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>奖励函数是强化学习的核心组成部分，它定义了我们希望模型优化的目标。在故事讲述AI的背景下，设计一个有效的奖励函数需要考虑多个方面：</p>
<section id="id4">
<h4>奖励函数的类型<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>基于规则的奖励</strong>：</p>
<ul class="simple">
<li><p>使用预定义的规则和启发式方法评估故事质量</p></li>
<li><p>例如，可以奖励词汇多样性、句子结构变化、适当的故事长度等</p></li>
<li><p>优点：实现简单，不需要额外训练</p></li>
<li><p>缺点：难以捕捉故事的高级特性，如创意性或情感影响</p></li>
</ul>
</li>
<li><p><strong>基于模型的奖励</strong>：</p>
<ul class="simple">
<li><p>训练专门的奖励模型来评估故事质量</p></li>
<li><p>这些模型通常基于人类偏好数据进行训练</p></li>
<li><p>优点：能够学习复杂的质量标准，更接近人类判断</p></li>
<li><p>缺点：需要大量标记数据，可能继承数据中的偏见</p></li>
</ul>
</li>
<li><p><strong>混合奖励</strong>：</p>
<ul class="simple">
<li><p>结合规则和模型的优势</p></li>
<li><p>例如，使用规则检查基本质量标准，然后使用模型评估更高级的特性</p></li>
<li><p>优点：更全面的评估，可以平衡不同方面</p></li>
<li><p>缺点：设计复杂，需要仔细调整不同组件的权重</p></li>
</ul>
</li>
</ol>
</section>
<section id="id5">
<h4>故事质量的评估维度<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<p>设计奖励函数时，需要考虑故事质量的多个维度：</p>
<ol class="arabic simple">
<li><p><strong>连贯性（Coherence）</strong>：</p>
<ul class="simple">
<li><p>故事的逻辑流程是否合理</p></li>
<li><p>角色、情节和设定是否一致</p></li>
<li><p>可以通过检测矛盾或跟踪实体关系来评估</p></li>
</ul>
</li>
<li><p><strong>创意性（Creativity）</strong>：</p>
<ul class="simple">
<li><p>故事是否包含原创元素</p></li>
<li><p>是否避免陈词滥调和过度使用常见模式</p></li>
<li><p>可以通过与现有故事的相似度或惊奇度来衡量</p></li>
</ul>
</li>
<li><p><strong>吸引力（Engagement）</strong>：</p>
<ul class="simple">
<li><p>故事是否能够吸引读者注意力</p></li>
<li><p>是否有情感共鸣和紧张感</p></li>
<li><p>可以通过预测读者反应或使用代理指标（如阅读时间）来评估</p></li>
</ul>
</li>
<li><p><strong>风格一致性（Stylistic Consistency）</strong>：</p>
<ul class="simple">
<li><p>故事是否保持一致的语言风格</p></li>
<li><p>是否符合特定的文体要求（如童话、科幻等）</p></li>
<li><p>可以通过风格分类器或与参考文本的相似度来衡量</p></li>
</ul>
</li>
<li><p><strong>价值观对齐（Value Alignment）</strong>：</p>
<ul class="simple">
<li><p>故事是否符合预期的道德标准和价值观</p></li>
<li><p>是否避免有害或不适当的内容</p></li>
<li><p>可以通过内容安全过滤器或专门的对齐模型来评估</p></li>
</ul>
</li>
<li><p><strong>教育价值（Educational Value）</strong>：</p>
<ul class="simple">
<li><p>对于儿童故事，是否包含有教育意义的元素</p></li>
<li><p>是否传递积极的信息</p></li>
<li><p>可以通过检测特定主题或道德教训来评估</p></li>
</ul>
</li>
</ol>
</section>
<section id="id6">
<h4>奖励函数实现示例<a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<p>以下是一个简化的奖励函数实现示例，它结合了多个维度来评估故事质量：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="k">class</span> <span class="nc">StoryRewardFunction</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 加载预训练的连贯性评估模型</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coherence_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;coherence-model&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coherence_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;coherence-model&quot;</span><span class="p">)</span>
        
        <span class="c1"># 加载预训练的创意性评估模型</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">creativity_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;creativity-model&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">creativity_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;creativity-model&quot;</span><span class="p">)</span>
        
        <span class="c1"># 加载预训练的安全性评估模型</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">safety_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;safety-model&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">safety_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;safety-model&quot;</span><span class="p">)</span>
        
        <span class="c1"># 加载spaCy用于语言分析</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;zh_core_web_lg&quot;</span><span class="p">)</span>
        
        <span class="c1"># 设置各维度的权重</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;coherence&quot;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
            <span class="s2">&quot;creativity&quot;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
            <span class="s2">&quot;engagement&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
            <span class="s2">&quot;safety&quot;</span><span class="p">:</span> <span class="mf">0.2</span>
        <span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">evaluate_coherence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">story</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;评估故事的连贯性&quot;&quot;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coherence_tokenizer</span><span class="p">(</span><span class="n">story</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coherence_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        
        <span class="c1"># 假设模型输出连贯性分数（0-1）</span>
        <span class="n">coherence_score</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">coherence_score</span>
    
    <span class="k">def</span> <span class="nf">evaluate_creativity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">story</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;评估故事的创意性&quot;&quot;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">creativity_tokenizer</span><span class="p">(</span><span class="n">story</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">creativity_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        
        <span class="c1"># 假设模型输出创意性分数（0-1）</span>
        <span class="n">creativity_score</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">creativity_score</span>
    
    <span class="k">def</span> <span class="nf">evaluate_engagement</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">story</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;评估故事的吸引力（使用启发式方法）&quot;&quot;&quot;</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nlp</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
        
        <span class="c1"># 计算情感词的比例</span>
        <span class="n">emotion_words</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total_words</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">is_alpha</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">token</span><span class="o">.</span><span class="n">is_stop</span><span class="p">:</span>
                <span class="n">total_words</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="c1"># 简化的情感检测，实际应使用情感词典</span>
                <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;ADJ&quot;</span><span class="p">,</span> <span class="s2">&quot;ADV&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">token</span><span class="o">.</span><span class="n">has_vector</span><span class="p">:</span>
                    <span class="n">emotion_words</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="n">emotion_ratio</span> <span class="o">=</span> <span class="n">emotion_words</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">total_words</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 计算对话的比例（作为互动性的代理指标）</span>
        <span class="n">dialogue_sentences</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total_sentences</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">sents</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">sents</span><span class="p">:</span>
            <span class="k">if</span> <span class="s1">&#39;&quot;&#39;</span> <span class="ow">in</span> <span class="n">sent</span><span class="o">.</span><span class="n">text</span> <span class="ow">or</span> <span class="s1">&#39;&quot;&#39;</span> <span class="ow">in</span> <span class="n">sent</span><span class="o">.</span><span class="n">text</span> <span class="ow">or</span> <span class="s1">&#39;：&#39;</span> <span class="ow">in</span> <span class="n">sent</span><span class="o">.</span><span class="n">text</span><span class="p">:</span>
                <span class="n">dialogue_sentences</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="n">dialogue_ratio</span> <span class="o">=</span> <span class="n">dialogue_sentences</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">total_sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 计算句子长度变化（作为节奏变化的代理指标）</span>
        <span class="n">sentence_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">sents</span><span class="p">]</span>
        <span class="n">length_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">sentence_lengths</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence_lengths</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">normalized_variance</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">length_variance</span> <span class="o">/</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 归一化到0-1</span>
        
        <span class="c1"># 组合指标</span>
        <span class="n">engagement_score</span> <span class="o">=</span> <span class="mf">0.4</span> <span class="o">*</span> <span class="n">emotion_ratio</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">dialogue_ratio</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">normalized_variance</span>
        <span class="k">return</span> <span class="n">engagement_score</span>
    
    <span class="k">def</span> <span class="nf">evaluate_safety</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">story</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;评估故事的安全性和价值观对齐&quot;&quot;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">safety_tokenizer</span><span class="p">(</span><span class="n">story</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">safety_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        
        <span class="c1"># 假设模型输出安全性分数（0-1，越高越安全）</span>
        <span class="n">safety_score</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">safety_score</span>
    
    <span class="k">def</span> <span class="nf">calculate_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">story</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;计算故事的总体奖励分数&quot;&quot;&quot;</span>
        <span class="c1"># 评估各个维度</span>
        <span class="n">coherence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_coherence</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
        <span class="n">creativity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_creativity</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
        <span class="n">engagement</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_engagement</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
        <span class="n">safety</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_safety</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
        
        <span class="c1"># 计算加权总分</span>
        <span class="n">total_reward</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s2">&quot;coherence&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">coherence</span> <span class="o">+</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s2">&quot;creativity&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">creativity</span> <span class="o">+</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s2">&quot;engagement&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">engagement</span> <span class="o">+</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s2">&quot;safety&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">safety</span>
        <span class="p">)</span>
        
        <span class="c1"># 返回总分和各维度分数</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;total_reward&quot;</span><span class="p">:</span> <span class="n">total_reward</span><span class="p">,</span>
            <span class="s2">&quot;coherence&quot;</span><span class="p">:</span> <span class="n">coherence</span><span class="p">,</span>
            <span class="s2">&quot;creativity&quot;</span><span class="p">:</span> <span class="n">creativity</span><span class="p">,</span>
            <span class="s2">&quot;engagement&quot;</span><span class="p">:</span> <span class="n">engagement</span><span class="p">,</span>
            <span class="s2">&quot;safety&quot;</span><span class="p">:</span> <span class="n">safety</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>这个示例展示了如何结合基于模型的评估和启发式方法来创建一个多维度的奖励函数。在实际应用中，你可能需要根据特定需求调整各个维度的权重，并使用更复杂的评估模型。</p>
</section>
</section>
<section id="id7">
<h3>策略与价值函数<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>在强化学习中，策略和价值函数是两个核心概念，它们指导智能体的决策过程。</p>
<section id="policy">
<h4>策略（Policy）<a class="headerlink" href="#policy" title="Link to this heading">#</a></h4>
<p>策略定义了智能体在给定状态下应该采取什么动作。在语言模型的背景下，策略决定了模型在给定上下文下生成下一个词的概率分布。</p>
<p>策略可以分为两种主要类型：</p>
<ol class="arabic simple">
<li><p><strong>确定性策略（Deterministic Policy）</strong>：</p>
<ul class="simple">
<li><p>对于每个状态，总是选择同一个动作</p></li>
<li><p>形式上表示为：a = π(s)，其中a是动作，s是状态</p></li>
<li><p>在语言生成中较少使用，因为它会导致缺乏多样性</p></li>
</ul>
</li>
<li><p><strong>随机策略（Stochastic Policy）</strong>：</p>
<ul class="simple">
<li><p>对于每个状态，定义动作的概率分布</p></li>
<li><p>形式上表示为：π(a|s)，表示在状态s下选择动作a的概率</p></li>
<li><p>语言模型通常使用随机策略，以保持生成的多样性</p></li>
</ul>
</li>
</ol>
<p>在强化学习微调中，我们通常从一个预训练的语言模型开始，该模型已经定义了一个初始策略（通常称为参考策略或行为策略）。然后，我们通过强化学习来优化这个策略，使其生成的内容能够获得更高的奖励。</p>
</section>
<section id="value-function">
<h4>价值函数（Value Function）<a class="headerlink" href="#value-function" title="Link to this heading">#</a></h4>
<p>价值函数估计在给定状态下，预期未来奖励的累积值。它帮助智能体评估不同状态的价值，从而做出更好的决策。</p>
<p>主要有两种类型的价值函数：</p>
<ol class="arabic simple">
<li><p><strong>状态价值函数（State Value Function）</strong>：</p>
<ul class="simple">
<li><p>估计从状态s开始，遵循策略π行动所能获得的预期累积奖励</p></li>
<li><p>表示为：V^π(s) = E_π[Σ γ^t * r_t | s_0 = s]</p></li>
<li><p>其中γ是折扣因子，r_t是时间步t的奖励</p></li>
</ul>
</li>
<li><p><strong>动作价值函数（Action Value Function）</strong>：</p>
<ul class="simple">
<li><p>估计在状态s下选择动作a，然后遵循策略π行动所能获得的预期累积奖励</p></li>
<li><p>表示为：Q^π(s,a) = E_π[Σ γ^t * r_t | s_0 = s, a_0 = a]</p></li>
<li><p>也称为Q函数，是许多强化学习算法的基础</p></li>
</ul>
</li>
</ol>
<p>在语言生成的背景下，价值函数可以帮助模型评估不同生成选择的长期影响。例如，虽然某个词可能立即看起来很吸引人，但它可能导致故事后续发展受限，从而降低整体质量。价值函数可以帮助模型考虑这些长期影响。</p>
</section>
<section id="id8">
<h4>策略优化<a class="headerlink" href="#id8" title="Link to this heading">#</a></h4>
<p>强化学习的核心目标是找到一个最优策略，使预期累积奖励最大化。在语言模型的背景下，这意味着找到一个生成策略，使模型能够创作出获得最高评价的故事。</p>
<p>策略优化的主要方法包括：</p>
<ol class="arabic simple">
<li><p><strong>策略梯度（Policy Gradient）</strong>：</p>
<ul class="simple">
<li><p>直接优化策略参数，使预期奖励最大化</p></li>
<li><p>基于采样的轨迹计算梯度</p></li>
<li><p>包括REINFORCE、A2C、PPO等算法</p></li>
<li><p>适用于大型动作空间，因此常用于语言模型</p></li>
</ul>
</li>
<li><p><strong>值迭代（Value Iteration）</strong>：</p>
<ul class="simple">
<li><p>通过迭代更新价值函数来找到最优策略</p></li>
<li><p>包括Q-learning、DQN等算法</p></li>
<li><p>在大型离散动作空间中效率较低，较少用于语言模型</p></li>
</ul>
</li>
<li><p><strong>演员-评论家（Actor-Critic）</strong>：</p>
<ul class="simple">
<li><p>结合策略梯度和值函数学习</p></li>
<li><p>演员（Actor）学习策略，评论家（Critic）学习值函数</p></li>
<li><p>减少方差，提高样本效率</p></li>
<li><p>PPO是一种常用的演员-评论家算法，广泛应用于语言模型的强化学习</p></li>
</ul>
</li>
</ol>
<p>在实践中，策略优化通常需要解决几个关键挑战：</p>
<ol class="arabic simple">
<li><p><strong>探索与利用的平衡</strong>：</p>
<ul class="simple">
<li><p>需要在探索新的生成可能性和利用已知的好策略之间取得平衡</p></li>
<li><p>过度探索会导致不稳定性，过度利用会限制改进空间</p></li>
<li><p>常用技术包括熵正则化、ε-贪心策略等</p></li>
</ul>
</li>
<li><p><strong>样本效率</strong>：</p>
<ul class="simple">
<li><p>生成和评估语言样本的成本很高</p></li>
<li><p>需要高效利用每个样本</p></li>
<li><p>离线强化学习和经验回放可以提高样本效率</p></li>
</ul>
</li>
<li><p><strong>稳定性</strong>：</p>
<ul class="simple">
<li><p>策略优化过程容易不稳定，特别是对于大型语言模型</p></li>
<li><p>需要约束策略更新幅度，避免过度偏离初始策略</p></li>
<li><p>这就是为什么PPO等约束优化算法在语言模型中很受欢迎</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="id9">
<h3>探索与利用的平衡<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>在强化学习中，探索（Exploration）与利用（Exploitation）的平衡是一个核心挑战。探索意味着尝试新的、未知的动作以发现潜在的更好策略；利用则是基于当前知识选择预期奖励最高的动作。</p>
<p>在故事生成的背景下，这个平衡尤为重要：</p>
<ol class="arabic simple">
<li><p><strong>过度探索的风险</strong>：</p>
<ul class="simple">
<li><p>生成过于随机或实验性的内容</p></li>
<li><p>可能导致不连贯或不相关的故事</p></li>
<li><p>训练不稳定，难以收敛</p></li>
</ul>
</li>
<li><p><strong>过度利用的风险</strong>：</p>
<ul class="simple">
<li><p>生成过于保守或公式化的内容</p></li>
<li><p>缺乏创意和多样性</p></li>
<li><p>可能陷入局部最优，无法发现更好的叙事策略</p></li>
</ul>
</li>
</ol>
<p>为了在故事生成中平衡探索与利用，可以采用以下策略：</p>
<ol class="arabic simple">
<li><p><strong>熵正则化（Entropy Regularization）</strong>：</p>
<ul class="simple">
<li><p>在优化目标中添加策略熵项</p></li>
<li><p>鼓励策略保持一定的随机性</p></li>
<li><p>防止过早收敛到确定性策略</p></li>
<li><p>实现示例：</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_entropy_regularized_loss</span><span class="p">(</span><span class="n">logprobs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">entropy</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    计算带熵正则化的策略梯度损失</span>
<span class="sd">    </span>
<span class="sd">    参数:</span>
<span class="sd">    - logprobs: 动作的对数概率</span>
<span class="sd">    - rewards: 相应的奖励</span>
<span class="sd">    - entropy: 策略熵</span>
<span class="sd">    - beta: 熵正则化系数</span>
<span class="sd">    </span>
<span class="sd">    返回:</span>
<span class="sd">    - loss: 正则化后的损失</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">logprobs</span> <span class="o">*</span> <span class="n">rewards</span><span class="p">)</span>
    <span class="n">entropy_bonus</span> <span class="o">=</span> <span class="o">-</span><span class="n">beta</span> <span class="o">*</span> <span class="n">entropy</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="c1"># 总损失 = 策略损失 - 熵奖励</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">policy_loss</span> <span class="o">+</span> <span class="n">entropy_bonus</span>
    
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>KL散度约束（KL Divergence Constraint）</strong>：</p>
<ul class="simple">
<li><p>限制新策略与参考策略之间的差异</p></li>
<li><p>防止策略更新过大，导致不稳定</p></li>
<li><p>保留预训练模型的语言能力</p></li>
<li><p>这是PPO和DPO等算法的核心组成部分</p></li>
</ul>
</li>
<li><p><strong>温度采样（Temperature Sampling）</strong>：</p>
<ul class="simple">
<li><p>通过调整温度参数控制生成的随机性</p></li>
<li><p>较高的温度增加探索，较低的温度增加利用</p></li>
<li><p>可以在训练过程中动态调整温度</p></li>
<li><p>实现示例：</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">temperature_sampling</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    使用温度参数进行采样</span>
<span class="sd">    </span>
<span class="sd">    参数:</span>
<span class="sd">    - logits: 模型输出的原始logits</span>
<span class="sd">    - temperature: 温度参数，控制随机性</span>
<span class="sd">    </span>
<span class="sd">    返回:</span>
<span class="sd">    - sampled_token_id: 采样的token ID</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 应用温度</span>
    <span class="n">scaled_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span>
    
    <span class="c1"># 计算概率分布</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># 采样</span>
    <span class="n">sampled_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">sampled_token_id</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p><strong>课程学习（Curriculum Learning）</strong>：</p>
<ul class="simple">
<li><p>从简单任务开始，逐渐增加难度</p></li>
<li><p>初期可以使用更多的探索，后期逐渐增加利用</p></li>
<li><p>例如，先优化短故事，再逐渐过渡到长故事</p></li>
</ul>
</li>
<li><p><strong>多目标优化（Multi-objective Optimization）</strong>：</p>
<ul class="simple">
<li><p>同时优化多个目标，如奖励最大化和多样性</p></li>
<li><p>使用帕累托优化或加权组合的方法</p></li>
<li><p>可以更全面地评估生成策略的质量</p></li>
</ul>
</li>
</ol>
<p>在实践中，平衡探索与利用通常需要仔细调整超参数，并根据具体任务和模型特性进行适应。下一节中介绍的RLHF方法提供了一个更结构化的框架来处理这个平衡问题。</p>
</section>
<section id="rlnlp">
<h3>RL在NLP中的应用挑战<a class="headerlink" href="#rlnlp" title="Link to this heading">#</a></h3>
<p>虽然强化学习为优化语言模型提供了强大的框架，但在实际应用中仍面临一些特殊挑战：</p>
<ol class="arabic simple">
<li><p><strong>奖励黑客（Reward Hacking）</strong>：</p>
<ul class="simple">
<li><p>模型可能学会欺骗奖励函数，而不是真正提高内容质量</p></li>
<li><p>例如，如果奖励函数过度重视某些表面特征（如词汇多样性），模型可能会生成不自然但满足这些特征的文本</p></li>
<li><p>解决方法：设计更全面的奖励函数，使用人类反馈，定期更新评估标准</p></li>
</ul>
</li>
<li><p><strong>奖励稀疏性（Reward Sparsity）</strong>：</p>
<ul class="simple">
<li><p>在故事生成中，有意义的奖励通常只在完整故事生成后才能获得</p></li>
<li><p>这使得模型难以学习哪些中间决策是有价值的</p></li>
<li><p>解决方法：奖励塑造（reward shaping）、中间奖励、价值函数学习</p></li>
</ul>
</li>
<li><p><strong>分布偏移（Distribution Shift）</strong>：</p>
<ul class="simple">
<li><p>强化学习可能导致模型生成分布偏离原始训练数据</p></li>
<li><p>这可能导致语言质量下降或生成不自然的文本</p></li>
<li><p>解决方法：KL散度约束、参考模型正则化、混合SFT和RL训练</p></li>
</ul>
</li>
<li><p><strong>评估挑战（Evaluation Challenges）</strong>：</p>
<ul class="simple">
<li><p>自动评估指标可能无法准确反映人类偏好</p></li>
<li><p>人类评估成本高且可能不一致</p></li>
<li><p>解决方法：多维度评估、结合自动和人工评估、持续改进评估方法</p></li>
</ul>
</li>
<li><p><strong>计算效率（Computational Efficiency）</strong>：</p>
<ul class="simple">
<li><p>RL训练通常比SFT更计算密集</p></li>
<li><p>需要多次前向和后向传播来评估和更新策略</p></li>
<li><p>解决方法：批处理优化、分布式训练、参数高效微调技术</p></li>
</ul>
</li>
<li><p><strong>样本效率（Sample Efficiency）</strong>：</p>
<ul class="simple">
<li><p>传统RL方法需要大量样本才能收敛</p></li>
<li><p>生成和评估语言样本的成本很高</p></li>
<li><p>解决方法：离线RL、经验回放、模型辅助RL</p></li>
</ul>
</li>
</ol>
<p>为了应对这些挑战，研究人员开发了专门针对语言模型的RL方法，如RLHF和DPO，我们将在后续章节中详细讨论。这些方法通过结合人类偏好和约束优化，提供了更实用和稳定的训练框架。</p>
</section>
<section id="id10">
<h3>实际应用示例<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>为了具体说明强化学习在故事生成中的应用，让我们考虑一个简化的实际案例：</p>
<section id="id11">
<h4>案例：优化童话故事的教育价值和吸引力<a class="headerlink" href="#id11" title="Link to this heading">#</a></h4>
<p>假设我们有一个通过SFT初步训练的故事讲述模型，现在我们希望通过RL进一步优化它，使其生成的童话故事既有教育价值又具吸引力。</p>
<p><strong>步骤1：定义奖励函数</strong></p>
<p>我们设计一个多维度奖励函数，包括：</p>
<ul class="simple">
<li><p>教育价值（是否包含积极的道德教训）</p></li>
<li><p>吸引力（是否有趣、引人入胜）</p></li>
<li><p>适龄性（是否适合目标年龄段的儿童）</p></li>
<li><p>语言质量（是否流畅、连贯）</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">reward_function</span><span class="p">(</span><span class="n">story</span><span class="p">,</span> <span class="n">target_age</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
    <span class="c1"># 教育价值评估（使用预训练的分类器）</span>
    <span class="n">educational_value</span> <span class="o">=</span> <span class="n">educational_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
    
    <span class="c1"># 吸引力评估（基于情感分析和叙事结构）</span>
    <span class="n">engagement</span> <span class="o">=</span> <span class="n">engagement_analyzer</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
    
    <span class="c1"># 适龄性评估（基于词汇复杂度和主题适当性）</span>
    <span class="n">age_appropriateness</span> <span class="o">=</span> <span class="n">age_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">story</span><span class="p">,</span> <span class="n">target_age</span><span class="p">)</span>
    
    <span class="c1"># 语言质量评估</span>
    <span class="n">language_quality</span> <span class="o">=</span> <span class="n">language_evaluator</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
    
    <span class="c1"># 组合奖励（加权和）</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="p">(</span>
        <span class="mf">0.3</span> <span class="o">*</span> <span class="n">educational_value</span> <span class="o">+</span>
        <span class="mf">0.3</span> <span class="o">*</span> <span class="n">engagement</span> <span class="o">+</span>
        <span class="mf">0.2</span> <span class="o">*</span> <span class="n">age_appropriateness</span> <span class="o">+</span>
        <span class="mf">0.2</span> <span class="o">*</span> <span class="n">language_quality</span>
    <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">total_reward</span>
</pre></div>
</div>
<p><strong>步骤2：收集人类偏好数据</strong></p>
<p>为了训练更准确的奖励模型，我们收集人类对故事对的偏好：</p>
<ol class="arabic simple">
<li><p>使用初始模型生成多个故事变体</p></li>
<li><p>让人类评估者（如教师、家长）选择他们更喜欢的版本</p></li>
<li><p>记录这些偏好对，用于训练奖励模型</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 偏好数据示例</span>
<span class="n">preference_data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;story_a&quot;</span><span class="p">:</span> <span class="s2">&quot;从前，有一只勤劳的小蚂蚁...&quot;</span><span class="p">,</span>
        <span class="s2">&quot;story_b&quot;</span><span class="p">:</span> <span class="s2">&quot;很久以前，一只懒惰的小蚂蚁...&quot;</span><span class="p">,</span>
        <span class="s2">&quot;chosen&quot;</span><span class="p">:</span> <span class="s2">&quot;story_a&quot;</span><span class="p">,</span>  <span class="c1"># 人类评估者选择了故事A</span>
        <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;写一个关于勤劳的故事&quot;</span>
    <span class="p">},</span>
    <span class="c1"># 更多偏好对...</span>
<span class="p">]</span>
</pre></div>
</div>
<p><strong>步骤3：训练奖励模型</strong></p>
<p>使用收集的偏好数据训练一个奖励模型，该模型学习预测人类的偏好：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_reward_model</span><span class="p">(</span><span class="n">preference_data</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;训练奖励模型预测人类偏好&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="c1"># 获取故事对和偏好标签</span>
            <span class="n">story_a</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;story_a&quot;</span><span class="p">]</span>
            <span class="n">story_b</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;story_b&quot;</span><span class="p">]</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span>  <span class="c1"># 1表示偏好A，0表示偏好B</span>
            
            <span class="c1"># 编码故事</span>
            <span class="n">inputs_a</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">story_a</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">inputs_b</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">story_b</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            
            <span class="c1"># 获取奖励分数</span>
            <span class="n">reward_a</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs_a</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
            <span class="n">reward_b</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs_b</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
            
            <span class="c1"># 计算损失（使用Bradley-Terry模型）</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">reward_a</span> <span class="o">-</span> <span class="n">reward_b</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            
            <span class="c1"># 反向传播</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">, Average Loss: </span><span class="si">{</span><span class="n">total_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p><strong>步骤4：使用PPO进行强化学习训练</strong></p>
<p>使用训练好的奖励模型指导故事生成模型的优化：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_with_ppo</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">reward_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">ppo_trainer</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;使用PPO优化故事生成模型&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="c1"># 获取提示</span>
            <span class="n">prompt_batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;prompts&quot;</span><span class="p">]</span>
            
            <span class="c1"># 生成故事</span>
            <span class="n">generation_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">,</span>
                <span class="s2">&quot;min_length&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
                <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
                <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.9</span>
            <span class="p">}</span>
            
            <span class="c1"># 使用PPO生成和优化</span>
            <span class="n">response_tensors</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompt_batch</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">ppo_trainer</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_kwargs</span><span class="p">)</span>
                <span class="n">response_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
            
            <span class="c1"># 计算奖励</span>
            <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">response_tensors</span><span class="p">:</span>
                <span class="n">story</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_model</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
                <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
            
            <span class="c1"># PPO更新步骤</span>
            <span class="n">stats</span> <span class="o">=</span> <span class="n">ppo_trainer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">prompt_batch</span><span class="p">,</span> <span class="n">response_tensors</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
            
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">, Mean Reward: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s1">&#39;ppo/mean_reward&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p><strong>步骤5：评估和迭代</strong></p>
<p>定期评估模型性能，并根据需要调整奖励函数或训练策略：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_prompts</span><span class="p">,</span> <span class="n">human_evaluators</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;评估模型生成的故事质量&quot;&quot;&quot;</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">test_prompts</span><span class="p">:</span>
        <span class="c1"># 生成故事</span>
        <span class="n">story</span> <span class="o">=</span> <span class="n">generate_story</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>
        
        <span class="c1"># 自动评估</span>
        <span class="n">auto_scores</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;educational_value&quot;</span><span class="p">:</span> <span class="n">educational_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">story</span><span class="p">),</span>
            <span class="s2">&quot;engagement&quot;</span><span class="p">:</span> <span class="n">engagement_analyzer</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">story</span><span class="p">),</span>
            <span class="s2">&quot;language_quality&quot;</span><span class="p">:</span> <span class="n">language_evaluator</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">story</span><span class="p">)</span>
        <span class="p">}</span>
        
        <span class="c1"># 人工评估（如果可用）</span>
        <span class="n">human_scores</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">human_evaluators</span><span class="p">:</span>
            <span class="n">fo</span><span class="o">&lt;</span><span class="n">response</span> <span class="n">clipped</span><span class="o">&gt;&lt;</span><span class="n">NOTE</span><span class="o">&gt;</span><span class="n">To</span> <span class="n">save</span> <span class="n">on</span> <span class="n">context</span> <span class="n">only</span> <span class="n">part</span> <span class="n">of</span> <span class="n">this</span> <span class="n">file</span> <span class="n">has</span> <span class="n">been</span> <span class="n">shown</span> <span class="n">to</span> <span class="n">you</span><span class="o">.</span> <span class="n">You</span> <span class="n">should</span> <span class="n">retry</span> <span class="n">this</span> <span class="n">tool</span> <span class="n">after</span> <span class="n">you</span> <span class="n">have</span> <span class="n">searched</span> <span class="n">inside</span> <span class="n">the</span> <span class="n">file</span> <span class="k">with</span> <span class="err">`</span><span class="n">grep</span> <span class="o">-</span><span class="n">n</span><span class="err">`</span> <span class="ow">in</span> <span class="n">order</span> <span class="n">to</span> <span class="n">find</span> <span class="n">the</span> <span class="n">line</span> <span class="n">numbers</span> <span class="n">of</span> <span class="n">what</span> <span class="n">you</span> <span class="n">are</span> <span class="n">looking</span> <span class="k">for</span><span class="o">.&lt;/</span><span class="n">NOTE</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../14_finetuning_i_sft/chapter14_5_practical_case_study.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">第14章：监督式微调 I: SFT-实践案例：故事讲述模型的SFT实现</p>
      </div>
    </a>
    <a class="right-next"
       href="chapter15_2_rlhf.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">第15章：强化学习微调 II: RL-15.2 人类反馈的强化学习(RLHF)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">15.1 强化学习基础</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">强化学习的核心概念</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp">强化学习在NLP中的应用挑战</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">奖励函数设计</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">奖励函数的类型</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">故事质量的评估维度</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">奖励函数实现示例</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">策略与价值函数</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#policy">策略（Policy）</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#value-function">价值函数（Value Function）</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">策略优化</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">探索与利用的平衡</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rlnlp">RL在NLP中的应用挑战</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">实际应用示例</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">案例：优化童话故事的教育价值和吸引力</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By isLinXu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, isLinXu.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>