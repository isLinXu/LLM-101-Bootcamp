
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>附录E：神经网络架构 &#8212; LLM-101创造营</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=40d2fe7a"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/mathjax_config.js?v=e9f8e615"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/appendix/E1_neural_network_architectures';</script>
    <link rel="icon" href="../../_static/panda.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="附录F：多模态基础" href="F1_multimodal.html" />
    <link rel="prev" title="附录D：深度学习框架" href="D1_deep_learning_frameworks.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/llm101.png" class="logo__image only-light" alt="LLM-101创造营 - Home"/>
    <script>document.write(`<img src="../../_static/llm101.png" class="logo__image only-dark" alt="LLM-101创造营 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">目录</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01_bigram/chapter01_bigram_language_model.html">第01章：Bigram语言模型（语言建模）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_micrograd/chapter02_micrograd.html">第02章：Micrograd（机器学习，反向传播）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_ngram_model/chapter03_ngram_model.html">第03章：N-gram模型（多层感知器，矩阵乘法，GELU激活函数）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_attention/chapter04_attention_model.html">第04章：注意力机制（Attention，Softmax，位置编码器）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_transformer/chapter05_transformer.html">第05章：Transformer（transformer架构，残差连接，层归一化，GPT-2）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06_tokenization/chaptet06_tokenization.html">第6章：分词技术(Tokenization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../07_optimization/chapter07_optimization.html">第7章：优化技术(Optimization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_need_for_speed_i_device/chapter08_need_for_speed_i_device.html">第8章：速度提升I：设备(Device)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../09_need_for_speed_ii_precision/chapter09_need_for_speed_ii_precision.html">第9章：速度提升II：精度(Precision)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../10_need_for_speed_iii_distributed/chapter10_need_for_speed_iii_distributed.html">第10章：速度提升III：分布式(Distributed)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../11_datasets/chapter11_datasets.html">第11章：数据集（Datasets）</a></li>

<li class="toctree-l1"><a class="reference internal" href="../12_inference_kv_cache/chapter12_inference_kv_cache.html">第12章：推理 I：KV缓存（KV-Cache）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../13_inference_quantization/chapter13_inference_quantization.html">第13章：推理 II：量化 (Quantization)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_1_supervised_finetuning_basics.html">第14章：监督式微调 I-SFT-14.1 监督式微调基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_2_parmeter_efficient_finetuning.html">第14章：监督式微调 I: SFT-14.1 监督式微调基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_3_lora_technique.html">第14章：监督式微调 I: SFT-14.3 LoRA技术详解</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_4_chat_model_finetuning.html">第14章：监督式微调 I: SFT-14.4 聊天模型的监督式微调</a></li>
<li class="toctree-l1"><a class="reference internal" href="../14_finetuning_i_sft/chapter14_5_practical_case_study.html">第14章：监督式微调 I: SFT-实践案例：故事讲述模型的SFT实现</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_Finetuning_ii_rl/chapter15_1_reinforcement_learning_basic.html">第15章：强化学习微调 II: RL-15.1 强化学习基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_Finetuning_ii_rl/chapter15_2_rlhf.html">第15章：强化学习微调 II: RL-15.2 人类反馈的强化学习(RLHF)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_Finetuning_ii_rl/chapter15_3_ppo_algorithm.html">第15章：强化学习微调 II: RL-15.3 近端策略优化(PPO)算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_Finetuning_ii_rl/chapter15_4_dpo_algorithm.html">第15章：强化学习微调 II: RL-## 15.4 直接偏好优化(DPO)算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16_deployment/chapter16_1_api_development.html">第16章：部署-16.1 API开发基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../16_deployment/chapter16_2_web_application.html">第16章：部署-16.2 Web应用开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_1_multimodal_basics.html">第17章：多模态-17.1 多模态基础理论</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_2_vqvae_technique.html">第17章：多模态-17.2 VQVAE技术详解</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_3_diffusion_transformer.html">第17章：多模态-17.3 扩散变换器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_4_lora_multimodal_training.html">第17章：多模态-基于LoRA的多模态模型训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../17_Multimodal/chapter17_5_multimodal_model_integration.html">第17章：多模态-17.5 多模态模型整合</a></li>
<li class="toctree-l1"><a class="reference internal" href="00_appendix_intro.html">附录</a></li>
<li class="toctree-l1"><a class="reference internal" href="A1_programming_languages.html">附录A：编程语言基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="B1_data_types.html">附录B：数据类型基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="C1_tensor_operations.html">附录C：张量操作基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="D1_deep_learning_frameworks.html">附录D：深度学习框架</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">附录E：神经网络架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="F1_multimodal.html">附录F：多模态基础</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/isLinXu/LLM-101-Bootcamp" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/LLM-101-Bootcamp/edit/main/chapters/appendix/E1_neural_network_architectures.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/LLM-101-Bootcamp/issues/new?title=Issue%20on%20page%20%2Fchapters/appendix/E1_neural_network_architectures.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/chapters/appendix/E1_neural_network_architectures.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>附录E：神经网络架构</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#e-1-gptllamamoe">E.1 神经网络架构：GPT、Llama、MoE及其演进</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">Transformer架构基础</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Transformer的核心组件</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Transformer的编码器-解码器结构</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-gpt-1gpt-4">GPT系列：从GPT-1到GPT-4</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-1">GPT-1：奠定基础</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-2">GPT-2：扩大规模</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-3">GPT-3：大规模预训练</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-4">GPT-4：多模态能力</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt">GPT系列的演进趋势</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llama">Llama系列：开放权重模型</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-1">Llama 1：高效架构</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-2">Llama 2：改进与开放</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3">Llama 3：最新进展</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Llama系列的关键创新</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moe">混合专家模型（MoE）</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">MoE的基本原理</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">MoE的优势</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">MoE的挑战</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">MoE在大型语言模型中的应用</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai">神经网络架构在故事讲述AI中的应用</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">基础生成能力</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">特定架构的优势</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">架构选择考虑因素</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">实现故事讲述系统的架构示例</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">未来架构趋势</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">总结</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="e">
<h1>附录E：神经网络架构<a class="headerlink" href="#e" title="Link to this heading">#</a></h1>
<section id="e-1-gptllamamoe">
<h2>E.1 神经网络架构：GPT、Llama、MoE及其演进<a class="headerlink" href="#e-1-gptllamamoe" title="Link to this heading">#</a></h2>
<p>在构建故事讲述AI大语言模型的过程中，理解不同的神经网络架构及其演进历程至关重要。本节将深入探讨几种主流的大型语言模型架构，包括GPT系列（1、2、3、4）、Llama系列及其创新组件（RoPE、RMSNorm、GQA），以及混合专家模型（MoE）。我们将分析这些架构的设计理念、核心创新和技术演进，以及它们在故事讲述AI系统中的应用。</p>
<section id="transformer">
<h3>Transformer架构基础<a class="headerlink" href="#transformer" title="Link to this heading">#</a></h3>
<p>在深入特定模型之前，我们需要回顾Transformer架构的基础，因为它是现代大型语言模型的基石。2017年，Vaswani等人在论文《Attention is All You Need》中提出了Transformer架构，彻底改变了自然语言处理领域。</p>
<section id="id1">
<h4>Transformer的核心组件<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<ol class="arabic">
<li><p><strong>自注意力机制（Self-Attention）</strong>：
允许模型在处理序列时考虑所有位置的信息，而不仅仅是相邻位置。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        
        <span class="c1"># 线性投影</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># 计算注意力分数</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="c1"># 应用掩码（如果提供）</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
            
        <span class="c1"># 注意力权重</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 应用注意力权重</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        
        <span class="c1"># 重塑和投影</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</li>
<li><p><strong>多头注意力（Multi-Head Attention）</strong>：
允许模型同时关注不同位置的不同表示子空间，增强模型的表达能力。</p></li>
<li><p><strong>位置编码（Positional Encoding）</strong>：
由于自注意力机制本身不包含位置信息，位置编码被添加到输入嵌入中，以提供序列中的位置信息。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_positional_encoding</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
    
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [1, seq_len, d_model]</span>
</pre></div>
</div>
</li>
<li><p><strong>前馈神经网络（Feed-Forward Network）</strong>：
每个Transformer层包含一个由两个线性变换和一个非线性激活函数组成的前馈网络。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
</pre></div>
</div>
</li>
<li><p><strong>层归一化（Layer Normalization）</strong>：
用于稳定深层网络的训练，通过归一化每一层的输入来减少内部协变量偏移。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
</pre></div>
</div>
</li>
<li><p><strong>残差连接（Residual Connections）</strong>：
每个子层的输出是其输入与子层函数应用于输入的结果之和，有助于训练非常深的网络。</p></li>
</ol>
</section>
<section id="id2">
<h4>Transformer的编码器-解码器结构<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<p>原始Transformer架构包含编码器和解码器两部分：</p>
<ol class="arabic simple">
<li><p><strong>编码器（Encoder）</strong>：</p>
<ul class="simple">
<li><p>由多个相同的层堆叠而成</p></li>
<li><p>每层包含多头自注意力机制和前馈神经网络</p></li>
<li><p>处理输入序列并生成上下文表示</p></li>
</ul>
</li>
<li><p><strong>解码器（Decoder）</strong>：</p>
<ul class="simple">
<li><p>同样由多个相同的层堆叠而成</p></li>
<li><p>每层包含多头自注意力、编码器-解码器注意力和前馈神经网络</p></li>
<li><p>生成输出序列，一次一个标记</p></li>
</ul>
</li>
</ol>
<p>现代大型语言模型通常只使用Transformer的一部分：</p>
<ul class="simple">
<li><p>GPT系列使用仅解码器架构（只有自回归解码器部分）</p></li>
<li><p>BERT使用仅编码器架构（只有双向编码器部分）</p></li>
<li><p>T5使用完整的编码器-解码器架构</p></li>
</ul>
</section>
</section>
<section id="gpt-gpt-1gpt-4">
<h3>GPT系列：从GPT-1到GPT-4<a class="headerlink" href="#gpt-gpt-1gpt-4" title="Link to this heading">#</a></h3>
<p>GPT（Generative Pre-trained Transformer）系列由OpenAI开发，代表了自回归语言模型的一个重要发展线路。</p>
<section id="gpt-1">
<h4>GPT-1：奠定基础<a class="headerlink" href="#gpt-1" title="Link to this heading">#</a></h4>
<p>GPT-1于2018年发布，是第一个展示大规模预训练加微调范式有效性的模型之一。</p>
<p><strong>核心特点</strong>：</p>
<ul class="simple">
<li><p>使用Transformer解码器架构</p></li>
<li><p>1.17亿参数</p></li>
<li><p>在大规模文本语料库上进行无监督预训练</p></li>
<li><p>通过微调适应下游任务</p></li>
</ul>
<p><strong>创新点</strong>：</p>
<ul class="simple">
<li><p>证明了预训练+微调范式的有效性</p></li>
<li><p>展示了Transformer在生成任务中的潜力</p></li>
<li><p>引入了”零样本”和”少样本”学习的概念</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPT-1风格的简化模型结构</span>
<span class="k">class</span> <span class="nc">GPT1</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        
        <span class="n">decoder_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TransformerDecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TransformerDecoder</span><span class="p">(</span><span class="n">decoder_layer</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 创建位置索引</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># 嵌入和位置编码</span>
        <span class="n">token_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">token_emb</span> <span class="o">+</span> <span class="n">pos_emb</span>
        
        <span class="c1"># 创建注意力掩码（确保只看到过去的标记）</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># 通过Transformer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        
        <span class="c1"># 输出层</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="gpt-2">
<h4>GPT-2：扩大规模<a class="headerlink" href="#gpt-2" title="Link to this heading">#</a></h4>
<p>GPT-2于2019年发布，代表了”规模化”思想的早期实践，展示了增加模型大小和训练数据可以显著提高性能。</p>
<p><strong>核心特点</strong>：</p>
<ul class="simple">
<li><p>架构与GPT-1类似，但规模更大</p></li>
<li><p>最大版本有15亿参数（比GPT-1大约10倍）</p></li>
<li><p>在更大、更多样化的数据集上训练</p></li>
<li><p>引入了更长的上下文窗口</p></li>
</ul>
<p><strong>创新点</strong>：</p>
<ul class="simple">
<li><p>证明了扩大模型规模可以带来性能提升</p></li>
<li><p>展示了零样本任务学习的强大能力</p></li>
<li><p>引入了更好的分词方法（Byte Pair Encoding）</p></li>
<li><p>改进的层归一化位置（移至每个子层的输入）</p></li>
</ul>
<p><strong>架构改进</strong>：</p>
<ul class="simple">
<li><p>将层归一化移至每个子层的输入（Pre-LN）</p></li>
<li><p>在最后一个自注意力块后添加额外的层归一化</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPT-2风格的层实现</span>
<span class="k">class</span> <span class="nc">GPT2Block</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># 自注意力块</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln_1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln_1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln_1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span>
        
        <span class="c1"># MLP块</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="gpt-3">
<h4>GPT-3：大规模预训练<a class="headerlink" href="#gpt-3" title="Link to this heading">#</a></h4>
<p>GPT-3于2020年发布，代表了大规模语言模型的重大飞跃，展示了”涌现能力”（emergent abilities）的概念。</p>
<p><strong>核心特点</strong>：</p>
<ul class="simple">
<li><p>1750亿参数（比GPT-2大约100倍）</p></li>
<li><p>96层Transformer解码器</p></li>
<li><p>12288维的嵌入</p></li>
<li><p>96个注意力头</p></li>
<li><p>在45TB文本数据上训练</p></li>
</ul>
<p><strong>创新点</strong>：</p>
<ul class="simple">
<li><p>展示了语言模型的涌现能力（如少样本学习）</p></li>
<li><p>证明了模型规模与性能之间的幂律关系</p></li>
<li><p>引入了”提示工程”（prompt engineering）的概念</p></li>
<li><p>展示了上下文学习（in-context learning）的能力</p></li>
</ul>
<p><strong>架构特点</strong>：</p>
<ul class="simple">
<li><p>与GPT-2基本相同，但规模更大</p></li>
<li><p>使用交替的稀疏注意力模式以提高效率</p></li>
<li><p>改进的初始化和归一化策略</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPT-3的稀疏注意力模式示例（简化版）</span>
<span class="k">def</span> <span class="nf">sparse_attention_pattern</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">pattern_type</span><span class="o">=</span><span class="s2">&quot;local&quot;</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">pattern_type</span> <span class="o">==</span> <span class="s2">&quot;local&quot;</span><span class="p">:</span>
        <span class="c1"># 局部注意力：每个标记只关注其前后窗口内的标记</span>
        <span class="n">window_size</span> <span class="o">=</span> <span class="mi">256</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">)</span>
            <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># +1因为我们不能看到未来</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            
    <span class="k">elif</span> <span class="n">pattern_type</span> <span class="o">==</span> <span class="s2">&quot;strided&quot;</span><span class="p">:</span>
        <span class="c1"># 跨步注意力：关注以固定步长采样的标记</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="mi">128</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>  <span class="c1"># +1因为我们不能看到未来</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                
    <span class="k">return</span> <span class="n">mask</span>
</pre></div>
</div>
</section>
<section id="gpt-4">
<h4>GPT-4：多模态能力<a class="headerlink" href="#gpt-4" title="Link to this heading">#</a></h4>
<p>GPT-4于2023年发布，代表了大型语言模型向多模态理解的扩展，以及更强的推理能力和更好的对齐性。</p>
<p><strong>核心特点</strong>：</p>
<ul class="simple">
<li><p>参数规模未公开，但估计超过1万亿</p></li>
<li><p>支持图像和文本输入（多模态）</p></li>
<li><p>更长的上下文窗口（最多支持32K标记）</p></li>
<li><p>更好的事实准确性和推理能力</p></li>
</ul>
<p><strong>创新点</strong>：</p>
<ul class="simple">
<li><p>多模态理解能力</p></li>
<li><p>更好的指令跟随和对齐</p></li>
<li><p>更强的推理和编码能力</p></li>
<li><p>减少了幻觉和偏见</p></li>
</ul>
<p><strong>架构特点</strong>：</p>
<ul class="simple">
<li><p>具体架构细节未公开</p></li>
<li><p>可能使用了混合专家系统（MoE）</p></li>
<li><p>可能整合了多种模态编码器</p></li>
<li><p>使用了更先进的对齐技术（RLHF和其他方法）</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPT-4多模态处理的概念示例</span>
<span class="k">class</span> <span class="nc">MultimodalGPT4</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_model</span><span class="p">,</span> <span class="n">vision_model</span><span class="p">,</span> <span class="n">fusion_layer</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_model</span> <span class="o">=</span> <span class="n">text_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span> <span class="o">=</span> <span class="n">vision_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fusion_layer</span> <span class="o">=</span> <span class="n">fusion_layer</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">image_input</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># 处理文本输入</span>
        <span class="k">if</span> <span class="n">text_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">text_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_model</span><span class="p">(</span><span class="n">text_input</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">text_features</span> <span class="o">=</span> <span class="kc">None</span>
            
        <span class="c1"># 处理图像输入</span>
        <span class="k">if</span> <span class="n">image_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">image_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_model</span><span class="p">(</span><span class="n">image_input</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">image_features</span> <span class="o">=</span> <span class="kc">None</span>
            
        <span class="c1"># 融合多模态特征</span>
        <span class="k">if</span> <span class="n">text_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">image_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># 多模态融合</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_layer</span><span class="p">(</span><span class="n">text_features</span><span class="p">,</span> <span class="n">image_features</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">text_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># 仅文本模式</span>
            <span class="k">return</span> <span class="n">text_features</span>
        <span class="k">elif</span> <span class="n">image_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># 仅图像模式</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_layer</span><span class="o">.</span><span class="n">text_projection</span><span class="p">(</span><span class="n">image_features</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;至少需要一种输入模态&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="gpt">
<h4>GPT系列的演进趋势<a class="headerlink" href="#gpt" title="Link to this heading">#</a></h4>
<p>GPT系列的演进展示了几个明显的趋势：</p>
<ol class="arabic simple">
<li><p><strong>规模扩大</strong>：</p>
<ul class="simple">
<li><p>参数数量：从1.17亿(GPT-1)到1750亿(GPT-3)再到估计的1万亿+(GPT-4)</p></li>
<li><p>训练数据量：从数GB到数TB</p></li>
<li><p>计算资源：从数百GPU小时到数百万GPU小时</p></li>
</ul>
</li>
<li><p><strong>能力涌现</strong>：</p>
<ul class="simple">
<li><p>随着规模增加，模型展示了未经专门训练的新能力</p></li>
<li><p>从简单的文本补全到复杂的推理和问题解决</p></li>
</ul>
</li>
<li><p><strong>多模态整合</strong>：</p>
<ul class="simple">
<li><p>从纯文本到文本+图像</p></li>
<li><p>未来可能包括更多模态（音频、视频等）</p></li>
</ul>
</li>
<li><p><strong>对齐改进</strong>：</p>
<ul class="simple">
<li><p>从纯语言建模到更好地遵循人类意图</p></li>
<li><p>减少有害输出和幻觉</p></li>
</ul>
</li>
<li><p><strong>架构优化</strong>：</p>
<ul class="simple">
<li><p>注意力机制的改进</p></li>
<li><p>更高效的训练和推理技术</p></li>
<li><p>更好的缩放策略</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="llama">
<h3>Llama系列：开放权重模型<a class="headerlink" href="#llama" title="Link to this heading">#</a></h3>
<p>Llama系列由Meta AI开发，代表了高性能开放权重模型的重要里程碑。Llama模型的开放性质促进了社区创新和模型适应的爆炸性增长。</p>
<section id="llama-1">
<h4>Llama 1：高效架构<a class="headerlink" href="#llama-1" title="Link to this heading">#</a></h4>
<p>Llama 1于2023年2月发布，展示了与闭源模型相当的性能，但使用更少的参数和计算资源。</p>
<p><strong>核心特点</strong>：</p>
<ul class="simple">
<li><p>提供多种规模：7B、13B、33B和65B参数</p></li>
<li><p>在1.4万亿标记上训练</p></li>
<li><p>使用更高效的架构组件</p></li>
<li><p>仅在公开可用的数据上训练</p></li>
</ul>
<p><strong>创新组件</strong>：</p>
<ol class="arabic">
<li><p><strong>旋转位置嵌入（RoPE, Rotary Positional Embedding）</strong>：
一种更有效的位置编码方法，通过旋转嵌入向量来编码位置信息。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="o">-</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">):</span>
    <span class="c1"># 获取查询和键的形状</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span>
    
    <span class="c1"># 获取位置编码</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">cos</span><span class="p">[</span><span class="n">position_ids</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># [bs, seq_len, 1, dim]</span>
    <span class="n">sin</span> <span class="o">=</span> <span class="n">sin</span><span class="p">[</span><span class="n">position_ids</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># [bs, seq_len, 1, dim]</span>
    
    <span class="c1"># 应用旋转</span>
    <span class="n">q_embed</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">*</span> <span class="n">cos</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotate_half</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">*</span> <span class="n">sin</span><span class="p">)</span>
    <span class="n">k_embed</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">cos</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotate_half</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">sin</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">q_embed</span><span class="p">,</span> <span class="n">k_embed</span>
</pre></div>
</div>
</li>
<li><p><strong>RMSNorm（Root Mean Square Normalization）</strong>：
一种简化的层归一化变体，计算更高效。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 计算RMS</span>
        <span class="n">rms</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="c1"># 归一化</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">rms</span>
        <span class="c1"># 缩放</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">x_norm</span>
</pre></div>
</div>
</li>
<li><p><strong>SwiGLU激活函数</strong>：
一种改进的门控线性单元变体，提供更好的性能。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SwiGLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w3</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</li>
</ol>
<p><strong>架构特点</strong>：</p>
<ul class="simple">
<li><p>预归一化Transformer架构</p></li>
<li><p>使用RoPE位置编码</p></li>
<li><p>使用RMSNorm代替LayerNorm</p></li>
<li><p>使用SwiGLU激活函数</p></li>
<li><p>没有偏置项（bias terms）</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Llama 1风格的Transformer块</span>
<span class="k">class</span> <span class="nc">LlamaBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="n">rope_theta</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="c1"># 注意力层</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        <span class="c1"># RoPE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rope_theta</span> <span class="o">=</span> <span class="n">rope_theta</span>
        
        <span class="c1"># 前馈层</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">SwiGLU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">):</span>
        <span class="c1"># 注意力块</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># 投影查询、键、值</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="c1"># 计算RoPE的sin和cos</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rope_theta</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">))</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">freqs</span><span class="p">)</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>
        <span class="n">cos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">freqs</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">freqs</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">cos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cos</span><span class="p">,</span> <span class="n">cos</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">sin</span><span class="p">,</span> <span class="n">sin</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 应用RoPE</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>
        
        <span class="c1"># 重塑为注意力计算的形状</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [batch, heads, seq_len, head_dim]</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># 计算注意力</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="c1"># 创建注意力掩码（确保只看到过去的标记）</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        
        <span class="c1"># 重塑和投影</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        
        <span class="c1"># 残差连接</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
        
        <span class="c1"># MLP块</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># 残差连接</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">x</span>
        
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="llama-2">
<h4>Llama 2：改进与开放<a class="headerlink" href="#llama-2" title="Link to this heading">#</a></h4>
<p>Llama 2于2023年7月发布，提供了性能改进和更开放的许可证，进一步推动了社区采用。</p>
<p><strong>核心特点</strong>：</p>
<ul class="simple">
<li><p>提供多种规模：7B、13B和70B参数</p></li>
<li><p>在2万亿标记上训练（比Llama 1多40%）</p></li>
<li><p>上下文长度从2048增加到4096标记</p></li>
<li><p>提供经过对话微调的变体（Llama 2 Chat）</p></li>
</ul>
<p><strong>架构改进</strong>：</p>
<ol class="arabic">
<li><p><strong>分组查询注意力（GQA, Grouped-Query Attention）</strong>：
一种介于多查询注意力（MQA）和多头注意力（MHA）之间的方法，提供更好的性能-效率权衡。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GroupedQueryAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_kv_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span> <span class="o">=</span> <span class="n">num_kv_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">//</span> <span class="n">num_kv_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        
        <span class="c1"># 投影查询、键、值</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="c1"># 重复键和值以匹配查询头数</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># 重塑为注意力计算的形状</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [batch, heads, seq_len, head_dim]</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># 计算注意力</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
            
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        
        <span class="c1"># 重塑和投影</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>改进的预训练目标</strong>：
使用更长的序列和更多样化的数据集。</p></li>
<li><p><strong>更好的对齐技术</strong>：
使用RLHF和其他技术使模型更好地对齐人类偏好。</p></li>
</ol>
<p><strong>Llama 2 Chat</strong>：
Llama 2还提供了经过对话微调的变体，使用了以下技术：</p>
<ul class="simple">
<li><p>监督微调（SFT）</p></li>
<li><p>人类反馈的强化学习（RLHF）</p></li>
<li><p>迭代的红队测试和改进</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Llama 2 Chat的系统提示示例</span>
<span class="n">SYSTEM_PROMPT</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are a helpful, harmless, and honest AI assistant. </span>
<span class="s2">You answer questions truthfully and don&#39;t make up information.</span>
<span class="s2">If you don&#39;t know the answer to a question, you admit it instead of making up an answer.</span>
<span class="s2">You refuse to engage in harmful, illegal, unethical, or deceptive activities.</span>
<span class="s2">You consider the safety and well-being of users in your responses.&quot;&quot;&quot;</span>

<span class="c1"># 对话格式</span>
<span class="k">def</span> <span class="nf">format_chat</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>
    <span class="n">formatted</span> <span class="o">=</span> <span class="n">SYSTEM_PROMPT</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
    <span class="k">for</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">msg</span><span class="p">[</span><span class="s2">&quot;role&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;user&quot;</span><span class="p">:</span>
            <span class="n">formatted</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;[USER]: </span><span class="si">{</span><span class="n">msg</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">msg</span><span class="p">[</span><span class="s2">&quot;role&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;assistant&quot;</span><span class="p">:</span>
            <span class="n">formatted</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;[ASSISTANT]: </span><span class="si">{</span><span class="n">msg</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">formatted</span> <span class="o">+=</span> <span class="s2">&quot;[ASSISTANT]: &quot;</span>
    <span class="k">return</span> <span class="n">formatted</span>
</pre></div>
</div>
</section>
<section id="llama-3">
<h4>Llama 3：最新进展<a class="headerlink" href="#llama-3" title="Link to this heading">#</a></h4>
<p>Llama 3于2024年4月发布，进一步提高了性能和能力。</p>
<p><strong>核心特点</strong>：</p>
<ul class="simple">
<li><p>提供8B和70B参数版本</p></li>
<li><p>更长的上下文窗口（支持8K标记）</p></li>
<li><p>改进的多语言能力</p></li>
<li><p>更好的指令跟随和对齐</p></li>
</ul>
<p><strong>架构改进</strong>：</p>
<ul class="simple">
<li><p>具体细节尚未完全公开</p></li>
<li><p>可能包括更高效的注意力机制</p></li>
<li><p>改进的训练方法和数据集</p></li>
<li><p>更好的对齐技术</p></li>
</ul>
</section>
<section id="id3">
<h4>Llama系列的关键创新<a class="headerlink" href="#id3" title="Link to this heading">#</a></h4>
<p>Llama系列引入了几项关键创新，这些创新已被广泛采用：</p>
<ol class="arabic simple">
<li><p><strong>旋转位置嵌入（RoPE）</strong>：</p>
<ul class="simple">
<li><p>通过旋转嵌入向量编码位置信息</p></li>
<li><p>提供更好的外推能力</p></li>
<li><p>计算效率高</p></li>
</ul>
</li>
<li><p><strong>RMSNorm</strong>：</p>
<ul class="simple">
<li><p>简化的层归一化变体</p></li>
<li><p>移除了均值计算和偏置项</p></li>
<li><p>提高计算效率</p></li>
</ul>
</li>
<li><p><strong>分组查询注意力（GQA）</strong>：</p>
<ul class="simple">
<li><p>在MQA和MHA之间取得平衡</p></li>
<li><p>减少内存使用和计算成本</p></li>
<li><p>保持模型质量</p></li>
</ul>
</li>
<li><p><strong>开放权重模型</strong>：</p>
<ul class="simple">
<li><p>促进社区创新和适应</p></li>
<li><p>允许本地部署和自定义</p></li>
<li><p>推动了大量微调变体的发展</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="moe">
<h3>混合专家模型（MoE）<a class="headerlink" href="#moe" title="Link to this heading">#</a></h3>
<p>混合专家模型（Mixture of Experts, MoE）是一种将模型容量与计算成本分离的架构，通过条件计算实现更高效的大型模型。</p>
<section id="id4">
<h4>MoE的基本原理<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>MoE的核心思想是将网络分解为多个”专家”（子网络），并使用一个路由器（gating network）决定每个输入应该由哪些专家处理。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MixtureOfExperts</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_experts</span> <span class="o">=</span> <span class="n">num_experts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>  <span class="c1"># 每个标记使用的专家数量</span>
        
        <span class="c1"># 创建专家</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experts</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">input_size</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_experts</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="c1"># 路由器（门控网络）</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">router</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        
        <span class="c1"># 计算路由分数</span>
        <span class="n">router_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">router</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [batch, seq, num_experts]</span>
        
        <span class="c1"># 选择前k个专家</span>
        <span class="n">routing_weights</span><span class="p">,</span> <span class="n">selected_experts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">router_logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">routing_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">routing_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 准备输出</span>
        <span class="n">final_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># 对每个专家进行计算</span>
        <span class="k">for</span> <span class="n">expert_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_experts</span><span class="p">):</span>
            <span class="c1"># 找到使用这个专家的位置</span>
            <span class="n">expert_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">selected_experts</span> <span class="o">==</span> <span class="n">expert_idx</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">expert_mask</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="k">continue</span>
                
            <span class="c1"># 提取需要这个专家处理的输入</span>
            <span class="n">expert_input</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">expert_mask</span>
            
            <span class="c1"># 应用专家</span>
            <span class="n">expert_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">experts</span><span class="p">[</span><span class="n">expert_idx</span><span class="p">](</span><span class="n">expert_input</span><span class="p">)</span>
            
            <span class="c1"># 找到这个专家的权重</span>
            <span class="n">weight_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">selected_experts</span> <span class="o">==</span> <span class="n">expert_idx</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">expert_weights</span> <span class="o">=</span> <span class="n">routing_weights</span> <span class="o">*</span> <span class="n">weight_mask</span>
            
            <span class="c1"># 加权求和</span>
            <span class="n">final_output</span> <span class="o">+=</span> <span class="n">expert_output</span> <span class="o">*</span> <span class="n">expert_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">final_output</span>
</pre></div>
</div>
</section>
<section id="id5">
<h4>MoE的优势<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>计算效率</strong>：</p>
<ul class="simple">
<li><p>每个输入只激活部分网络</p></li>
<li><p>可以增加模型容量而不等比增加计算</p></li>
</ul>
</li>
<li><p><strong>专业化</strong>：</p>
<ul class="simple">
<li><p>不同专家可以专注于不同类型的输入</p></li>
<li><p>提高模型处理多样化数据的能力</p></li>
</ul>
</li>
<li><p><strong>可扩展性</strong>：</p>
<ul class="simple">
<li><p>可以通过添加更多专家轻松扩展模型</p></li>
<li><p>支持分布式训练和推理</p></li>
</ul>
</li>
</ol>
</section>
<section id="id6">
<h4>MoE的挑战<a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>负载平衡</strong>：</p>
<ul class="simple">
<li><p>确保专家被均匀使用</p></li>
<li><p>防止”专家崩溃”（某些专家永不被使用）</p></li>
</ul>
</li>
<li><p><strong>路由决策</strong>：</p>
<ul class="simple">
<li><p>设计有效的路由算法</p></li>
<li><p>平衡路由计算成本和准确性</p></li>
</ul>
</li>
<li><p><strong>训练稳定性</strong>：</p>
<ul class="simple">
<li><p>MoE模型可能更难训练</p></li>
<li><p>需要特殊的正则化和初始化技术</p></li>
</ul>
</li>
</ol>
</section>
<section id="id7">
<h4>MoE在大型语言模型中的应用<a class="headerlink" href="#id7" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Switch Transformers</strong>：
Google的早期MoE实现，每个标记只路由到一个专家。</p></li>
<li><p><strong>GShard</strong>：
Google的分布式MoE框架，用于训练超大模型。</p></li>
<li><p><strong>Mixtral 8x7B</strong>：
Mistral AI的MoE模型，使用8个7B专家，每次激活2个。</p></li>
<li><p><strong>GPT-4</strong>：
据推测使用了MoE架构，但具体细节未公开。</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Mixtral风格的MoE层示例</span>
<span class="k">class</span> <span class="nc">MixtralMoELayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_experts</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_experts_per_tok</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_experts</span> <span class="o">=</span> <span class="n">num_experts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_experts_per_tok</span> <span class="o">=</span> <span class="n">num_experts_per_tok</span>
        
        <span class="c1"># 创建专家</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experts</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">SwiGLU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_experts</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="c1"># 路由器</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">router</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="c1"># 计算路由分数</span>
        <span class="n">router_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">router</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">routing_weights</span><span class="p">,</span> <span class="n">selected_experts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
            <span class="n">router_logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_experts_per_tok</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">routing_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">routing_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 初始化输出</span>
        <span class="n">final_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># 应用专家</span>
        <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">seq_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_experts_per_tok</span><span class="p">):</span>
                    <span class="n">expert_idx</span> <span class="o">=</span> <span class="n">selected_experts</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">seq_idx</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="n">weight</span> <span class="o">=</span> <span class="n">routing_weights</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">seq_idx</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    
                    <span class="c1"># 应用专家并加权</span>
                    <span class="n">expert_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">experts</span><span class="p">[</span><span class="n">expert_idx</span><span class="p">](</span><span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">seq_idx</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
                    <span class="n">final_output</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">seq_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">expert_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                    
        <span class="k">return</span> <span class="n">final_output</span>
</pre></div>
</div>
</section>
</section>
<section id="ai">
<h3>神经网络架构在故事讲述AI中的应用<a class="headerlink" href="#ai" title="Link to this heading">#</a></h3>
<p>不同的神经网络架构在构建故事讲述AI系统中有各自的优势和应用场景。</p>
<section id="id8">
<h4>基础生成能力<a class="headerlink" href="#id8" title="Link to this heading">#</a></h4>
<p>大型语言模型的基础生成能力是故事讲述的核心：</p>
<ol class="arabic simple">
<li><p><strong>上下文理解</strong>：</p>
<ul class="simple">
<li><p>理解用户提供的故事提示和约束</p></li>
<li><p>保持长篇故事的连贯性和一致性</p></li>
</ul>
</li>
<li><p><strong>创意生成</strong>：</p>
<ul class="simple">
<li><p>创造引人入胜的情节和角色</p></li>
<li><p>生成多样化和原创的内容</p></li>
</ul>
</li>
<li><p><strong>风格适应</strong>：</p>
<ul class="simple">
<li><p>模仿不同的文学风格和流派</p></li>
<li><p>调整语言复杂性以适应不同的受众</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 故事生成示例（使用预训练模型）</span>
<span class="k">def</span> <span class="nf">generate_story</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># 生成参数</span>
    <span class="n">gen_kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="n">max_length</span><span class="p">,</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="n">temperature</span><span class="p">,</span>
        <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="s2">&quot;top_k&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="s2">&quot;repetition_penalty&quot;</span><span class="p">:</span> <span class="mf">1.2</span><span class="p">,</span>
        <span class="s2">&quot;no_repeat_ngram_size&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span>
    <span class="p">}</span>
    
    <span class="c1"># 生成故事</span>
    <span class="n">output_sequences</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">gen_kwargs</span><span class="p">)</span>
    
    <span class="c1"># 解码</span>
    <span class="n">story</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">story</span>
</pre></div>
</div>
</section>
<section id="id9">
<h4>特定架构的优势<a class="headerlink" href="#id9" title="Link to this heading">#</a></h4>
<p>不同架构在故事讲述中有不同的优势：</p>
<ol class="arabic simple">
<li><p><strong>GPT系列</strong>：</p>
<ul class="simple">
<li><p>强大的通用生成能力</p></li>
<li><p>丰富的文化和知识背景</p></li>
<li><p>GPT-4的多模态能力可用于图文故事</p></li>
</ul>
</li>
<li><p><strong>Llama系列</strong>：</p>
<ul class="simple">
<li><p>开放权重允许本地部署和自定义</p></li>
<li><p>高效架构适合资源受限环境</p></li>
<li><p>社区微调变体提供专业化能力</p></li>
</ul>
</li>
<li><p><strong>MoE模型</strong>：</p>
<ul class="simple">
<li><p>可以同时掌握多种写作风格</p></li>
<li><p>不同专家可以专注于不同类型的故事</p></li>
<li><p>更高效地处理长篇故事</p></li>
</ul>
</li>
</ol>
</section>
<section id="id10">
<h4>架构选择考虑因素<a class="headerlink" href="#id10" title="Link to this heading">#</a></h4>
<p>在为故事讲述AI选择架构时，应考虑以下因素：</p>
<ol class="arabic simple">
<li><p><strong>故事复杂性</strong>：</p>
<ul class="simple">
<li><p>简单故事可能不需要最大的模型</p></li>
<li><p>复杂、长篇故事可能需要更大的上下文窗口</p></li>
</ul>
</li>
<li><p><strong>互动性要求</strong>：</p>
<ul class="simple">
<li><p>实时互动需要更高效的推理</p></li>
<li><p>回合制互动可以容忍更长的生成时间</p></li>
</ul>
</li>
<li><p><strong>部署环境</strong>：</p>
<ul class="simple">
<li><p>本地部署可能需要更小、更高效的模型</p></li>
<li><p>云部署可以使用更大的模型</p></li>
</ul>
</li>
<li><p><strong>自定义需求</strong>：</p>
<ul class="simple">
<li><p>需要特定领域适应的场景可能更适合开放权重模型</p></li>
<li><p>需要多模态能力的场景可能更适合GPT-4等模型</p></li>
</ul>
</li>
</ol>
</section>
<section id="id11">
<h4>实现故事讲述系统的架构示例<a class="headerlink" href="#id11" title="Link to this heading">#</a></h4>
<p>以下是一个基于Llama 2实现故事讲述系统的架构示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">StorytellerSystem</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="n">model_path</span><span class="p">,</span>
            <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
            <span class="n">device_map</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        
        <span class="c1"># 故事生成参数</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">default_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
            <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
            <span class="s2">&quot;top_k&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
            <span class="s2">&quot;repetition_penalty&quot;</span><span class="p">:</span> <span class="mf">1.2</span><span class="p">,</span>
            <span class="s2">&quot;max_length&quot;</span><span class="p">:</span> <span class="mi">2048</span>
        <span class="p">}</span>
        
        <span class="c1"># 故事提示模板</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">story_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        Write a creative and engaging story based on the following elements:</span>
<span class="s2">        </span>
<span class="s2">        Setting: </span><span class="si">{setting}</span>
<span class="s2">        Main Character: </span><span class="si">{character}</span>
<span class="s2">        Theme: </span><span class="si">{theme}</span>
<span class="s2">        Genre: </span><span class="si">{genre}</span>
<span class="s2">        </span>
<span class="s2">        The story should be </span><span class="si">{tone}</span><span class="s2"> in tone and approximately </span><span class="si">{length}</span><span class="s2"> words long.</span>
<span class="s2">        &quot;&quot;&quot;</span>
        
    <span class="k">def</span> <span class="nf">generate_story</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">story_elements</span><span class="p">,</span> <span class="n">custom_params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># 准备提示</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">story_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">story_elements</span><span class="p">)</span>
        
        <span class="c1"># 合并参数</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_params</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">custom_params</span><span class="p">:</span>
            <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">custom_params</span><span class="p">)</span>
            
        <span class="c1"># 编码提示</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># 生成故事</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">output_sequences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
                <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;max_length&quot;</span><span class="p">],</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;temperature&quot;</span><span class="p">],</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;top_p&quot;</span><span class="p">],</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;top_k&quot;</span><span class="p">],</span>
                <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;repetition_penalty&quot;</span><span class="p">],</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            
        <span class="c1"># 解码故事</span>
        <span class="n">story</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># 移除提示部分</span>
        <span class="n">story</span> <span class="o">=</span> <span class="n">story</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):]</span>
        
        <span class="k">return</span> <span class="n">story</span>
        
    <span class="k">def</span> <span class="nf">continue_story</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">story_so_far</span><span class="p">,</span> <span class="n">continuation_hint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">custom_params</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># 准备提示</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">story_so_far</span>
        <span class="k">if</span> <span class="n">continuation_hint</span><span class="p">:</span>
            <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Continue the story with the following elements: </span><span class="si">{</span><span class="n">continuation_hint</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            
        <span class="c1"># 合并参数</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_params</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">custom_params</span><span class="p">:</span>
            <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">custom_params</span><span class="p">)</span>
            
        <span class="c1"># 编码提示</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># 生成续写</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">output_sequences</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
                <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="mi">500</span><span class="p">,</span>  <span class="c1"># 生成500个新标记</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;temperature&quot;</span><span class="p">],</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;top_p&quot;</span><span class="p">],</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;top_k&quot;</span><span class="p">],</span>
                <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;repetition_penalty&quot;</span><span class="p">],</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            
        <span class="c1"># 解码续写</span>
        <span class="n">continuation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># 移除提示部分</span>
        <span class="n">continuation</span> <span class="o">=</span> <span class="n">continuation</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):]</span>
        
        <span class="k">return</span> <span class="n">continuation</span>
</pre></div>
</div>
</section>
</section>
<section id="id12">
<h3>未来架构趋势<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<p>神经网络架构的发展仍在快速演进，以下是一些可能影响故事讲述AI未来的趋势：</p>
<ol class="arabic simple">
<li><p><strong>更长的上下文窗口</strong>：</p>
<ul class="simple">
<li><p>允许生成和理解更长的故事</p></li>
<li><p>改进的注意力机制（如滑动窗口、稀疏注意力）</p></li>
</ul>
</li>
<li><p><strong>多模态整合</strong>：</p>
<ul class="simple">
<li><p>文本和图像的深度融合</p></li>
<li><p>添加音频和视频能力</p></li>
</ul>
</li>
<li><p><strong>更高效的架构</strong>：</p>
<ul class="simple">
<li><p>更多采用MoE和条件计算</p></li>
<li><p>更好的参数共享和知识蒸馏</p></li>
</ul>
</li>
<li><p><strong>可控生成</strong>：</p>
<ul class="simple">
<li><p>更精细的风格和内容控制</p></li>
<li><p>更好的约束满足能力</p></li>
</ul>
</li>
<li><p><strong>个性化和适应</strong>：</p>
<ul class="simple">
<li><p>更容易适应用户偏好</p></li>
<li><p>持续学习和改进</p></li>
</ul>
</li>
</ol>
</section>
<section id="id13">
<h3>总结<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>神经网络架构是故事讲述AI系统的基础，不同的架构提供了不同的能力和权衡：</p>
<ol class="arabic simple">
<li><p><strong>GPT系列</strong>代表了大规模预训练和多模态能力的发展路线，从GPT-1的基础模型到GPT-4的多模态理解，展示了规模和数据如何带来能力的涌现。</p></li>
<li><p><strong>Llama系列</strong>展示了开放权重模型和高效架构的价值，通过创新组件如RoPE、RMSNorm和GQA提高了性能和效率。</p></li>
<li><p>**混合专家模型（MoE）**提供了一种扩展模型容量而不等比增加计算成本的方法，特别适合需要多样化能力的故事讲述系统。</p></li>
</ol>
<p>在构建故事讲述AI系统时，理解这些架构的优势和局限性至关重要。随着技术的发展，我们可以期待更强大、更高效、更易于控制的模型架构，为创造引人入胜的故事体验提供更好的基础。</p>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="D1_deep_learning_frameworks.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">附录D：深度学习框架</p>
      </div>
    </a>
    <a class="right-next"
       href="F1_multimodal.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">附录F：多模态基础</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#e-1-gptllamamoe">E.1 神经网络架构：GPT、Llama、MoE及其演进</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">Transformer架构基础</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Transformer的核心组件</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Transformer的编码器-解码器结构</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-gpt-1gpt-4">GPT系列：从GPT-1到GPT-4</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-1">GPT-1：奠定基础</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-2">GPT-2：扩大规模</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-3">GPT-3：大规模预训练</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-4">GPT-4：多模态能力</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt">GPT系列的演进趋势</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llama">Llama系列：开放权重模型</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-1">Llama 1：高效架构</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-2">Llama 2：改进与开放</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#llama-3">Llama 3：最新进展</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Llama系列的关键创新</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#moe">混合专家模型（MoE）</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">MoE的基本原理</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">MoE的优势</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">MoE的挑战</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">MoE在大型语言模型中的应用</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai">神经网络架构在故事讲述AI中的应用</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">基础生成能力</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">特定架构的优势</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">架构选择考虑因素</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">实现故事讲述系统的架构示例</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">未来架构趋势</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">总结</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By isLinXu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, isLinXu.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>